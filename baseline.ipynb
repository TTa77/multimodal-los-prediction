{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Frozen Bert Weights for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalDataset, MultimodalNetwork, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  5 11:56:10 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 43%   56C    P8    21W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_train = pd.read_csv('data/regression/static_train.csv')\n",
    "static_val = pd.read_csv('data/regression/static_val.csv')\n",
    "static_test = pd.read_csv('data/regression/static_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/notes_with_interval.csv')\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])]\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])]\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "dynamic_train = dynamic[dynamic['id'].isin(static_train['id'])].copy()\n",
    "dynamic_val = dynamic[dynamic['id'].isin(static_val['id'])].copy()\n",
    "dynamic_test = dynamic[dynamic['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>-0.724520</td>\n",
       "      <td>-0.293495</td>\n",
       "      <td>-1.025798</td>\n",
       "      <td>-0.653624</td>\n",
       "      <td>1.399389</td>\n",
       "      <td>-0.747178</td>\n",
       "      <td>-0.556081</td>\n",
       "      <td>0.707111</td>\n",
       "      <td>-0.902117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28478629</td>\n",
       "      <td>10/8/96 5:30</td>\n",
       "      <td>0.367784</td>\n",
       "      <td>0.627490</td>\n",
       "      <td>1.820680</td>\n",
       "      <td>-1.186258</td>\n",
       "      <td>-0.682355</td>\n",
       "      <td>0.668018</td>\n",
       "      <td>-0.025601</td>\n",
       "      <td>-0.350448</td>\n",
       "      <td>0.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22195489</td>\n",
       "      <td>9/18/45 21:05</td>\n",
       "      <td>0.367784</td>\n",
       "      <td>0.995884</td>\n",
       "      <td>0.120187</td>\n",
       "      <td>2.968290</td>\n",
       "      <td>-0.543572</td>\n",
       "      <td>2.292872</td>\n",
       "      <td>-0.308524</td>\n",
       "      <td>0.354591</td>\n",
       "      <td>-0.055675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28478629</td>\n",
       "      <td>10/13/96 4:35</td>\n",
       "      <td>0.149323</td>\n",
       "      <td>2.285262</td>\n",
       "      <td>1.783712</td>\n",
       "      <td>-0.653624</td>\n",
       "      <td>-1.792618</td>\n",
       "      <td>0.039042</td>\n",
       "      <td>-0.721119</td>\n",
       "      <td>-0.350448</td>\n",
       "      <td>0.367546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21525925</td>\n",
       "      <td>2/10/45 5:40</td>\n",
       "      <td>-0.287598</td>\n",
       "      <td>-0.109298</td>\n",
       "      <td>-1.136699</td>\n",
       "      <td>-0.547097</td>\n",
       "      <td>0.566691</td>\n",
       "      <td>-0.852007</td>\n",
       "      <td>-0.367466</td>\n",
       "      <td>0.178332</td>\n",
       "      <td>-0.761044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0  26115624    9/7/50 0:22 -0.724520    -0.293495 -1.025798 -0.653624   \n",
       "3  28478629   10/8/96 5:30  0.367784     0.627490  1.820680 -1.186258   \n",
       "4  22195489  9/18/45 21:05  0.367784     0.995884  0.120187  2.968290   \n",
       "5  28478629  10/13/96 4:35  0.149323     2.285262  1.783712 -0.653624   \n",
       "6  21525925   2/10/45 5:40 -0.287598    -0.109298 -1.136699 -0.547097   \n",
       "\n",
       "   chloride  creatinine   glucose    sodium  potassium  \n",
       "0  1.399389   -0.747178 -0.556081  0.707111  -0.902117  \n",
       "3 -0.682355    0.668018 -0.025601 -0.350448   0.649693  \n",
       "4 -0.543572    2.292872 -0.308524  0.354591  -0.055675  \n",
       "5 -1.792618    0.039042 -0.721119 -0.350448   0.367546  \n",
       "6  0.566691   -0.852007 -0.367466  0.178332  -0.761044  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic_train.loc[:, features] = scaler.fit_transform(dynamic_train[features])\n",
    "dynamic_val.loc[:, features] = scaler.transform(dynamic_val[features])\n",
    "dynamic_test.loc[:, features] = scaler.transform(dynamic_test[features])  \n",
    "\n",
    "dynamic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic train cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20015730    [[-0.9429806989713342, 0.2590960935569942, -0....\n",
       "20020562    [[-0.9429806989713342, 0.07489917607824341, -0...\n",
       "20022465    [[0.3677840064001248, -0.477691576358009, -0.3...\n",
       "20024788    [[1.2416271433144308, -0.8460854113155106, 1.6...\n",
       "20025344    [[-0.0691375620570282, 0.6274899285144958, 2.5...\n",
       "                                  ...                        \n",
       "29988601    [[0.8047055748572778, -0.2934946588792582, 0.5...\n",
       "29990184    [[-0.0691375620570282, 1.9168683508657516, 0.1...\n",
       "29990494    [[0.8047055748572778, -0.6618884938367599, -0....\n",
       "29992506    [[-0.7245199147427577, 0.44329301103574503, -0...\n",
       "29994296    [[-0.5060591305141812, -0.8460854113155106, 1....\n",
       "Length: 2547, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_train = dynamic_train['id'].value_counts().to_dict()\n",
    "dynamic_train = dynamic_train.sort_values(by=['id', 'charttime'])\n",
    "dynamic_train = dynamic_train.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_train['id']).agg(list)\n",
    "\n",
    "dynamic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic val cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20008098    [[0.5862447906287013, -0.8460854113155106, -0....\n",
       "20013244    [[-0.5060591305141812, 0.6274899285144958, -0....\n",
       "20021110    [[0.1493232221715483, 0.6274899285144958, -0.1...\n",
       "20022095    [[4.518538906743078, -3.240645338539271, -0.76...\n",
       "20081858    [[0.3677840064001248, -1.9512669161880154, 0.0...\n",
       "                                  ...                        \n",
       "29797354    [[-0.9429806989713342, -0.1092977414005074, 0....\n",
       "29877242    [[-0.2875983462856047, -0.1092977414005074, -0...\n",
       "29890924    [[-0.5060591305141812, -0.1092977414005074, -0...\n",
       "29961119    [[-1.5983630516570637, -0.2934946588792582, -0...\n",
       "29970938    [[-0.2875983462856047, -0.1092977414005074, -0...\n",
       "Length: 284, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_val = dynamic_val['id'].value_counts().to_dict()\n",
    "dynamic_val = dynamic_val.sort_values(by=['id', 'charttime'])\n",
    "dynamic_val = dynamic_val.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_val['id']).agg(list)\n",
    "\n",
    "dynamic_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic test cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20024177    [[0.5862447906287013, 0.07489917607824341, -0....\n",
       "20048978    [[1.0231663590858544, -0.8460854113155106, 1.0...\n",
       "20130544    [[0.1493232221715483, -0.2934946588792582, -0....\n",
       "20176432    [[-1.379902267428487, 1.1800806809507483, -0.9...\n",
       "20205345    [[0.5862447906287013, 0.44329301103574503, -0....\n",
       "                                  ...                        \n",
       "29891413    [[-1.8168238358856401, -0.1092977414005074, -0...\n",
       "29923363    [[-0.0691375620570282, -0.2934946588792582, -1...\n",
       "29935333    [[-0.2875983462856047, 1.364277598429499, -0.2...\n",
       "29981653    [[-0.5060591305141812, 0.44329301103574503, -0...\n",
       "29997500    [[-0.7245199147427577, 2.6536560207807547, -0....\n",
       "Length: 315, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_test = dynamic_test['id'].value_counts().to_dict()\n",
    "dynamic_test = dynamic_test.sort_values(by=['id', 'charttime'])\n",
    "dynamic_test = dynamic_test.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_test['id']).agg(list)\n",
    "\n",
    "dynamic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])].copy()\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])].copy()\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultimodalDataset(static=static_train, dynamic=dynamic_train, id_lengths=id_lengths_train, notes=notes_train)\n",
    "validation_data = MultimodalDataset(static=static_val, dynamic=dynamic_val, id_lengths=id_lengths_val, notes=notes_val)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=1500, shuffle=True, collate_fn=collation)\n",
    "val_loader = DataLoader(validation_data, batch_size=150, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "text_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for params in text_model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "model = MultimodalNetwork(input_size=9, out_features=1, hidden_size=128, text_model=text_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  5 11:56:28 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 31%   52C    P2    76W / 300W |   1641MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: [1/200]\n",
      "step: [1/2] | loss: 58.87\n",
      "step: [2/2] | loss: 53.32\n",
      "Training epoch loss: 56.0983\n",
      "\n",
      "validation epoch: [1/200]\n",
      "Validation epoch loss: 93.2310\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [2/200]\n",
      "step: [1/2] | loss: 54.4\n",
      "step: [2/2] | loss: 41.35\n",
      "Training epoch loss: 47.8727\n",
      "\n",
      "validation epoch: [2/200]\n",
      "Validation epoch loss: 83.4951\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [3/200]\n",
      "step: [1/2] | loss: 43.82\n",
      "step: [2/2] | loss: 41.52\n",
      "Training epoch loss: 42.6701\n",
      "\n",
      "validation epoch: [3/200]\n",
      "Validation epoch loss: 78.4478\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [4/200]\n",
      "step: [1/2] | loss: 31.73\n",
      "step: [2/2] | loss: 50.79\n",
      "Training epoch loss: 41.2601\n",
      "\n",
      "validation epoch: [4/200]\n",
      "Validation epoch loss: 77.9065\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [5/200]\n",
      "step: [1/2] | loss: 37.33\n",
      "step: [2/2] | loss: 42.04\n",
      "Training epoch loss: 39.6836\n",
      "\n",
      "validation epoch: [5/200]\n",
      "Validation epoch loss: 77.1575\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [6/200]\n",
      "step: [1/2] | loss: 36.26\n",
      "step: [2/2] | loss: 46.23\n",
      "Training epoch loss: 41.2460\n",
      "\n",
      "validation epoch: [6/200]\n",
      "Validation epoch loss: 78.0666\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [7/200]\n",
      "step: [1/2] | loss: 36.72\n",
      "step: [2/2] | loss: 48.08\n",
      "Training epoch loss: 42.4020\n",
      "\n",
      "validation epoch: [7/200]\n",
      "Validation epoch loss: 77.0194\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [8/200]\n",
      "step: [1/2] | loss: 37.42\n",
      "step: [2/2] | loss: 47.5\n",
      "Training epoch loss: 42.4636\n",
      "\n",
      "validation epoch: [8/200]\n",
      "Validation epoch loss: 76.3441\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [9/200]\n",
      "step: [1/2] | loss: 44.49\n",
      "step: [2/2] | loss: 35.81\n",
      "Training epoch loss: 40.1515\n",
      "\n",
      "validation epoch: [9/200]\n",
      "Validation epoch loss: 76.9863\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [10/200]\n",
      "step: [1/2] | loss: 38.75\n",
      "step: [2/2] | loss: 41.84\n",
      "Training epoch loss: 40.2989\n",
      "\n",
      "validation epoch: [10/200]\n",
      "Validation epoch loss: 76.9771\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [11/200]\n",
      "step: [1/2] | loss: 40.34\n",
      "step: [2/2] | loss: 37.82\n",
      "Training epoch loss: 39.0824\n",
      "\n",
      "validation epoch: [11/200]\n",
      "Validation epoch loss: 76.3077\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [12/200]\n",
      "step: [1/2] | loss: 35.72\n",
      "step: [2/2] | loss: 43.87\n",
      "Training epoch loss: 39.7975\n",
      "\n",
      "validation epoch: [12/200]\n",
      "Validation epoch loss: 76.5009\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [13/200]\n",
      "step: [1/2] | loss: 38.05\n",
      "step: [2/2] | loss: 40.84\n",
      "Training epoch loss: 39.4478\n",
      "\n",
      "validation epoch: [13/200]\n",
      "Validation epoch loss: 77.6991\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [14/200]\n",
      "step: [1/2] | loss: 42.85\n",
      "step: [2/2] | loss: 34.54\n",
      "Training epoch loss: 38.6952\n",
      "\n",
      "validation epoch: [14/200]\n",
      "Validation epoch loss: 78.9324\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [15/200]\n",
      "step: [1/2] | loss: 39.1\n",
      "step: [2/2] | loss: 40.28\n",
      "Training epoch loss: 39.6860\n",
      "\n",
      "validation epoch: [15/200]\n",
      "Validation epoch loss: 79.4385\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [16/200]\n",
      "step: [1/2] | loss: 34.4\n",
      "step: [2/2] | loss: 46.85\n",
      "Training epoch loss: 40.6288\n",
      "\n",
      "validation epoch: [16/200]\n",
      "Validation epoch loss: 79.3269\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [17/200]\n",
      "step: [1/2] | loss: 37.77\n",
      "step: [2/2] | loss: 41.51\n",
      "Training epoch loss: 39.6422\n",
      "\n",
      "validation epoch: [17/200]\n",
      "Validation epoch loss: 76.8141\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [18/200]\n",
      "step: [1/2] | loss: 41.42\n",
      "step: [2/2] | loss: 35.76\n",
      "Training epoch loss: 38.5918\n",
      "\n",
      "validation epoch: [18/200]\n",
      "Validation epoch loss: 77.3388\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [19/200]\n",
      "step: [1/2] | loss: 33.56\n",
      "step: [2/2] | loss: 46.7\n",
      "Training epoch loss: 40.1328\n",
      "\n",
      "validation epoch: [19/200]\n",
      "Validation epoch loss: 76.5872\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [20/200]\n",
      "step: [1/2] | loss: 37.26\n",
      "step: [2/2] | loss: 41.38\n",
      "Training epoch loss: 39.3208\n",
      "\n",
      "validation epoch: [20/200]\n",
      "Validation epoch loss: 78.2958\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [21/200]\n",
      "step: [1/2] | loss: 38.94\n",
      "step: [2/2] | loss: 39.13\n",
      "Training epoch loss: 39.0334\n",
      "\n",
      "validation epoch: [21/200]\n",
      "Validation epoch loss: 76.3660\n",
      "\n",
      "No improvement over 10 epochs\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "patience = 10\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'training epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "\n",
    "        loss = criterion(outputs, los)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % max(1, round(len(train_loader) * 0.1)) == 0:\n",
    "            print(f'step: [{step+1}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step+1 == 1 and epoch == 1:\n",
    "                with open('data/losses/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open('data/losses/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'Training epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open('data/losses/training_loss_epoch.txt', 'w') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open('data/losses/training_loss_epoch.txt', 'a') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print(f'validation epoch: [{epoch}/{epochs}]')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss_epoch = 0\n",
    "\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            packed_dynamic_X_val, notes_X_val, notes_intervals_val, los_val = val_batch\n",
    "\n",
    "            packed_dynamic_X_val = packed_dynamic_X_val.to(device)\n",
    "            los_val = los_val.to(device)\n",
    "\n",
    "            notes_X_val_gpu = []\n",
    "            for notes in notes_X_val:\n",
    "                notes_val_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "                notes_X_val_gpu.append(notes_val_gpu)\n",
    "\n",
    "            val_outputs = model(packed_dynamic_X_val, notes_X_val_gpu, notes_intervals_val)\n",
    "            val_loss = criterion(val_outputs, los_val)\n",
    "            validation_loss_epoch += val_loss\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "        print(f'Validation epoch loss: {avg_validation_loss.item():.4f}\\n')\n",
    "        \n",
    "        if len(validation_loss) == 0 or (avg_validation_loss.item() < min(validation_loss)):\n",
    "            stagnation = 0\n",
    "            torch.save(model.state_dict(), 'data/models/baseline_regression_best_model.pth')\n",
    "            print(f'new minimum validation loss')\n",
    "            print(f'model saved\\n')\n",
    "\n",
    "        else:\n",
    "            stagnation += 1\n",
    "\n",
    "        validation_loss.append(avg_validation_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            with open('data/losses/validation_loss_epoch.txt', 'w') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "        else:\n",
    "            with open('data/losses/validation_loss_epoch.txt', 'a') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "        if stagnation >= patience:\n",
    "            print(f'No improvement over {patience} epochs')\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('===============================\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
