{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalDataset, MultimodalNetwork, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 31 16:01:23 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 37%   55C    P8    24W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>id</th>\n",
       "      <th>note_type</th>\n",
       "      <th>note_seq</th>\n",
       "      <th>charttime</th>\n",
       "      <th>text</th>\n",
       "      <th>icu_death</th>\n",
       "      <th>interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17915608-RR-64</td>\n",
       "      <td>20008098</td>\n",
       "      <td>RR</td>\n",
       "      <td>64</td>\n",
       "      <td>1975-02-06 16:23:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17915608-RR-65</td>\n",
       "      <td>20008098</td>\n",
       "      <td>RR</td>\n",
       "      <td>65</td>\n",
       "      <td>1975-02-07 10:50:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17915608-RR-66</td>\n",
       "      <td>20008098</td>\n",
       "      <td>RR</td>\n",
       "      <td>66</td>\n",
       "      <td>1975-02-07 20:17:00</td>\n",
       "      <td>INDICATION:  ___ year old man s/p RUL lobectom...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17915608-RR-67</td>\n",
       "      <td>20008098</td>\n",
       "      <td>RR</td>\n",
       "      <td>67</td>\n",
       "      <td>1975-02-08 12:20:00</td>\n",
       "      <td>INDICATION:  ___ year old man s/p VATS to Open...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17915608-RR-68</td>\n",
       "      <td>20008098</td>\n",
       "      <td>RR</td>\n",
       "      <td>68</td>\n",
       "      <td>1975-02-09 07:26:00</td>\n",
       "      <td>INDICATION:  ___ year old man s/p open RUL lob...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          note_id        id note_type  note_seq            charttime  \\\n",
       "0  17915608-RR-64  20008098        RR        64  1975-02-06 16:23:00   \n",
       "1  17915608-RR-65  20008098        RR        65  1975-02-07 10:50:00   \n",
       "2  17915608-RR-66  20008098        RR        66  1975-02-07 20:17:00   \n",
       "3  17915608-RR-67  20008098        RR        67  1975-02-08 12:20:00   \n",
       "4  17915608-RR-68  20008098        RR        68  1975-02-09 07:26:00   \n",
       "\n",
       "                                                text  icu_death  interval  \n",
       "0  EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...          0         3  \n",
       "1  EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...          0         2  \n",
       "2  INDICATION:  ___ year old man s/p RUL lobectom...          0         2  \n",
       "3  INDICATION:  ___ year old man s/p VATS to Open...          0         1  \n",
       "4  INDICATION:  ___ year old man s/p open RUL lob...          0         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = pd.read_csv('data/notes_with_interval.csv')\n",
    "\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21792938</td>\n",
       "      <td>4/13/28 14:18</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28398464</td>\n",
       "      <td>12/9/34 8:10</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>93.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28478629</td>\n",
       "      <td>10/8/96 5:30</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>142.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22195489</td>\n",
       "      <td>9/18/45 21:05</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>118.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate   bun  calcium  chloride  \\\n",
       "0  26115624    9/7/50 0:22      12.0         22.0   9.0      7.9     111.0   \n",
       "1  21792938  4/13/28 14:18      11.0         28.0  46.0      7.1      96.0   \n",
       "2  28398464   12/9/34 8:10      18.0         21.0  24.0     10.3     100.0   \n",
       "3  28478629   10/8/96 5:30      17.0         27.0  86.0      7.4      96.0   \n",
       "4  22195489  9/18/45 21:05      17.0         29.0  40.0     11.3      97.0   \n",
       "\n",
       "   creatinine  glucose  sodium  potassium  \n",
       "0         0.6     97.0   141.0        3.6  \n",
       "1         5.0     95.0   130.0        5.2  \n",
       "2         0.7     93.0   135.0        4.0  \n",
       "3         3.3    142.0   135.0        4.7  \n",
       "4         6.4    118.0   139.0        4.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "\n",
    "dynamic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hosp_admittime</th>\n",
       "      <th>hosp_dischtime</th>\n",
       "      <th>icu_intime</th>\n",
       "      <th>icu_outtime</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>icu_death</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>...</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>copd</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>ihd</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21999692</td>\n",
       "      <td>2/20/77 21:08</td>\n",
       "      <td>3/6/77 16:40</td>\n",
       "      <td>2/22/77 16:35</td>\n",
       "      <td>2/25/77 17:54</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK/AFRICAN AMERICAN</td>\n",
       "      <td>55.139306</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25936663</td>\n",
       "      <td>1/11/45 23:02</td>\n",
       "      <td>1/22/45 16:06</td>\n",
       "      <td>1/12/45 15:10</td>\n",
       "      <td>1/13/45 18:50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>65.032170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25675339</td>\n",
       "      <td>11/2/83 21:12</td>\n",
       "      <td>11/19/83 15:36</td>\n",
       "      <td>11/6/83 20:59</td>\n",
       "      <td>11/10/83 0:44</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>61.837481</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>27993048</td>\n",
       "      <td>11/19/67 8:23</td>\n",
       "      <td>12/25/67 14:53</td>\n",
       "      <td>11/26/67 16:26</td>\n",
       "      <td>12/5/67 16:53</td>\n",
       "      <td>9.02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>56.881238</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>22987108</td>\n",
       "      <td>6/10/46 16:37</td>\n",
       "      <td>7/12/46 0:00</td>\n",
       "      <td>6/22/46 11:46</td>\n",
       "      <td>7/13/46 0:27</td>\n",
       "      <td>20.53</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>69.439961</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id hosp_admittime  hosp_dischtime      icu_intime    icu_outtime  \\\n",
       "11  21999692  2/20/77 21:08    3/6/77 16:40   2/22/77 16:35  2/25/77 17:54   \n",
       "22  25936663  1/11/45 23:02   1/22/45 16:06   1/12/45 15:10  1/13/45 18:50   \n",
       "25  25675339  11/2/83 21:12  11/19/83 15:36   11/6/83 20:59  11/10/83 0:44   \n",
       "30  27993048  11/19/67 8:23  12/25/67 14:53  11/26/67 16:26  12/5/67 16:53   \n",
       "32  22987108  6/10/46 16:37    7/12/46 0:00   6/22/46 11:46   7/13/46 0:27   \n",
       "\n",
       "    los_icu  icu_death gender                    race  admission_age  ...  \\\n",
       "11     3.05          0      M  BLACK/AFRICAN AMERICAN      55.139306  ...   \n",
       "22     1.15          0      M                   WHITE      65.032170  ...   \n",
       "25     3.16          0      M                   WHITE      61.837481  ...   \n",
       "30     9.02          0      F                   WHITE      56.881238  ...   \n",
       "32    20.53          1      M                   WHITE      69.439961  ...   \n",
       "\n",
       "    atrial_fibrillation  malignant_cancer chf ckd  cld  copd  diabetes  \\\n",
       "11                    0                 0   0   0    0     1         0   \n",
       "22                    0                 1   0   0    1     0         0   \n",
       "25                    0                 0   0   0    0     0         0   \n",
       "30                    0                 0   1   1    0     0         1   \n",
       "32                    1                 0   0   1    0     0         0   \n",
       "\n",
       "    hypertension  ihd  stroke  \n",
       "11             0    0       0  \n",
       "22             1    1       0  \n",
       "25             0    0       0  \n",
       "30             1    1       0  \n",
       "32             1    1       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static = pd.read_csv('data/static.csv')\n",
    "static = static[static['id'].isin(dynamic['id'])]\n",
    "static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcome(row, source_df):\n",
    "    working_id = row['id']\n",
    "    outcome = source_df[source_df['id'] == working_id].icu_death.iloc[0]\n",
    "\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "      <th>icu_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21792938</td>\n",
       "      <td>4/13/28 14:18</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28398464</td>\n",
       "      <td>12/9/34 8:10</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>93.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28478629</td>\n",
       "      <td>10/8/96 5:30</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>142.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22195489</td>\n",
       "      <td>9/18/45 21:05</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>118.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate   bun  calcium  chloride  \\\n",
       "0  26115624    9/7/50 0:22      12.0         22.0   9.0      7.9     111.0   \n",
       "1  21792938  4/13/28 14:18      11.0         28.0  46.0      7.1      96.0   \n",
       "2  28398464   12/9/34 8:10      18.0         21.0  24.0     10.3     100.0   \n",
       "3  28478629   10/8/96 5:30      17.0         27.0  86.0      7.4      96.0   \n",
       "4  22195489  9/18/45 21:05      17.0         29.0  40.0     11.3      97.0   \n",
       "\n",
       "   creatinine  glucose  sodium  potassium  icu_death  \n",
       "0         0.6     97.0   141.0        3.6          0  \n",
       "1         5.0     95.0   130.0        5.2          0  \n",
       "2         0.7     93.0   135.0        4.0          1  \n",
       "3         3.3    142.0   135.0        4.7          0  \n",
       "4         6.4    118.0   139.0        4.2          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic['icu_death'] = dynamic.apply(lambda row: get_outcome(row, static), axis=1)\n",
    "\n",
    "dynamic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "      <th>icu_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>-0.718835</td>\n",
       "      <td>-0.298273</td>\n",
       "      <td>-1.014269</td>\n",
       "      <td>-0.640887</td>\n",
       "      <td>1.416042</td>\n",
       "      <td>-0.746533</td>\n",
       "      <td>-0.560555</td>\n",
       "      <td>0.721273</td>\n",
       "      <td>-0.895593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21792938</td>\n",
       "      <td>4/13/28 14:18</td>\n",
       "      <td>-0.936561</td>\n",
       "      <td>0.813247</td>\n",
       "      <td>0.363490</td>\n",
       "      <td>-1.492120</td>\n",
       "      <td>-0.689949</td>\n",
       "      <td>1.595494</td>\n",
       "      <td>-0.584288</td>\n",
       "      <td>-1.243671</td>\n",
       "      <td>1.357400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28398464</td>\n",
       "      <td>12/9/34 8:10</td>\n",
       "      <td>0.587520</td>\n",
       "      <td>-0.483526</td>\n",
       "      <td>-0.455718</td>\n",
       "      <td>1.912812</td>\n",
       "      <td>-0.128351</td>\n",
       "      <td>-0.693305</td>\n",
       "      <td>-0.608022</td>\n",
       "      <td>-0.350515</td>\n",
       "      <td>-0.332345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28478629</td>\n",
       "      <td>10/8/96 5:30</td>\n",
       "      <td>0.369794</td>\n",
       "      <td>0.627994</td>\n",
       "      <td>1.852959</td>\n",
       "      <td>-1.172908</td>\n",
       "      <td>-0.689949</td>\n",
       "      <td>0.690620</td>\n",
       "      <td>-0.026548</td>\n",
       "      <td>-0.350515</td>\n",
       "      <td>0.653340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22195489</td>\n",
       "      <td>9/18/45 21:05</td>\n",
       "      <td>0.369794</td>\n",
       "      <td>0.998501</td>\n",
       "      <td>0.140070</td>\n",
       "      <td>2.976854</td>\n",
       "      <td>-0.549549</td>\n",
       "      <td>2.340684</td>\n",
       "      <td>-0.311352</td>\n",
       "      <td>0.364010</td>\n",
       "      <td>-0.050720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0  26115624    9/7/50 0:22 -0.718835    -0.298273 -1.014269 -0.640887   \n",
       "1  21792938  4/13/28 14:18 -0.936561     0.813247  0.363490 -1.492120   \n",
       "2  28398464   12/9/34 8:10  0.587520    -0.483526 -0.455718  1.912812   \n",
       "3  28478629   10/8/96 5:30  0.369794     0.627994  1.852959 -1.172908   \n",
       "4  22195489  9/18/45 21:05  0.369794     0.998501  0.140070  2.976854   \n",
       "\n",
       "   chloride  creatinine   glucose    sodium  potassium  icu_death  \n",
       "0  1.416042   -0.746533 -0.560555  0.721273  -0.895593          0  \n",
       "1 -0.689949    1.595494 -0.584288 -1.243671   1.357400          0  \n",
       "2 -0.128351   -0.693305 -0.608022 -0.350515  -0.332345          1  \n",
       "3 -0.689949    0.690620 -0.026548 -0.350515   0.653340          0  \n",
       "4 -0.549549    2.340684 -0.311352  0.364010  -0.050720          1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic[features] = scaler.fit_transform(dynamic[features])\n",
    "\n",
    "dynamic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>text</th>\n",
       "      <th>interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20008098</td>\n",
       "      <td>1975-02-06 16:23:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20008098</td>\n",
       "      <td>1975-02-07 10:50:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20008098</td>\n",
       "      <td>1975-02-07 20:17:00</td>\n",
       "      <td>INDICATION:  ___ year old man s/p RUL lobectom...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20008098</td>\n",
       "      <td>1975-02-08 12:20:00</td>\n",
       "      <td>INDICATION:  ___ year old man s/p VATS to Open...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20008098</td>\n",
       "      <td>1975-02-09 07:26:00</td>\n",
       "      <td>INDICATION:  ___ year old man s/p open RUL lob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            charttime  \\\n",
       "0  20008098  1975-02-06 16:23:00   \n",
       "1  20008098  1975-02-07 10:50:00   \n",
       "2  20008098  1975-02-07 20:17:00   \n",
       "3  20008098  1975-02-08 12:20:00   \n",
       "4  20008098  1975-02-09 07:26:00   \n",
       "\n",
       "                                                text  interval  \n",
       "0  EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...         3  \n",
       "1  EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...         2  \n",
       "2  INDICATION:  ___ year old man s/p RUL lobectom...         2  \n",
       "3  INDICATION:  ___ year old man s/p VATS to Open...         1  \n",
       "4  INDICATION:  ___ year old man s/p open RUL lob...         1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes['icu_death'] = notes.apply(lambda row: get_outcome(row, static), axis=1)\n",
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20008098    [[0.5875197296491911, -0.8540328822821452, -0....\n",
       "20013244    [[-0.501109573269091, 0.6279939835876895, -0.4...\n",
       "20015730    [[-0.9365612944364038, 0.2574872671202309, -0....\n",
       "20020562    [[-0.9365612944364038, 0.07223390888650152, -0...\n",
       "20021110    [[0.15206800848187826, 0.6279939835876895, -0....\n",
       "                                  ...                        \n",
       "29990184    [[-0.06565785210177813, 1.924767491223795, 0.2...\n",
       "29990494    [[0.8052455902328475, -0.6687795240484159, -0....\n",
       "29992506    [[-0.7188354338527474, 0.44274062535396025, -0...\n",
       "29994296    [[-0.501109573269091, -0.8540328822821452, 1.4...\n",
       "29997500    [[-0.7188354338527474, 2.6657809241587125, -0....\n",
       "Length: 3146, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths = dynamic['id'].value_counts().to_dict()\n",
    "dynamic = dynamic.sort_values(by=['id', 'charttime'])\n",
    "dynamic = dynamic.apply(lambda x: list(x[features]), axis=1).groupby(dynamic['id']).agg(list)\n",
    "\n",
    "dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultimodalDataset(static=static, dynamic=dynamic, id_lengths=id_lengths, notes=notes)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = MultimodalNetwork(input_size=9, out_features=1, hidden_size=128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 31 16:01:55 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   53C    P2    80W / 300W |   1641MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [1/5]\n",
      "step: [0/1573] | loss: 4.244\n",
      "step: [50/1573] | loss: 7.21\n",
      "step: [100/1573] | loss: 8.383\n",
      "step: [150/1573] | loss: 29.6\n",
      "step: [200/1573] | loss: 37.03\n",
      "step: [250/1573] | loss: 15.95\n",
      "step: [300/1573] | loss: 195.1\n",
      "step: [350/1573] | loss: 461.2\n",
      "step: [400/1573] | loss: 15.41\n",
      "step: [450/1573] | loss: 321.3\n",
      "step: [500/1573] | loss: 105.2\n",
      "step: [550/1573] | loss: 6.917\n",
      "step: [600/1573] | loss: 1.412\n",
      "step: [650/1573] | loss: 84.04\n",
      "step: [700/1573] | loss: 8.559\n",
      "step: [750/1573] | loss: 5.281\n",
      "step: [800/1573] | loss: 8.33\n",
      "step: [850/1573] | loss: 2.586\n",
      "step: [900/1573] | loss: 34.06\n",
      "step: [950/1573] | loss: 15.84\n",
      "step: [1000/1573] | loss: 3.315\n",
      "step: [1050/1573] | loss: 7.359\n",
      "step: [1100/1573] | loss: 373.5\n",
      "step: [1150/1573] | loss: 125.2\n",
      "step: [1200/1573] | loss: 121.7\n",
      "step: [1250/1573] | loss: 17.9\n",
      "step: [1300/1573] | loss: 57.5\n",
      "step: [1350/1573] | loss: 3.868\n",
      "step: [1400/1573] | loss: 9.39\n",
      "step: [1450/1573] | loss: 11.65\n",
      "step: [1500/1573] | loss: 5.402\n",
      "step: [1550/1573] | loss: 77.79\n",
      "epoch loss: 71.5423\n",
      "\n",
      "===============================\n",
      "\n",
      "epoch: [2/5]\n",
      "step: [0/1573] | loss: 44.13\n",
      "step: [50/1573] | loss: 185.1\n",
      "step: [100/1573] | loss: 65.59\n",
      "step: [150/1573] | loss: 18.56\n",
      "step: [200/1573] | loss: 2.175\n",
      "step: [250/1573] | loss: 26.16\n",
      "step: [300/1573] | loss: 72.27\n",
      "step: [350/1573] | loss: 27.42\n",
      "step: [400/1573] | loss: 249.9\n",
      "step: [450/1573] | loss: 4.247\n",
      "step: [500/1573] | loss: 6.46\n",
      "step: [550/1573] | loss: 9.481\n",
      "step: [600/1573] | loss: 6.294\n",
      "step: [650/1573] | loss: 8.765\n",
      "step: [700/1573] | loss: 10.22\n",
      "step: [750/1573] | loss: 4.302\n",
      "step: [800/1573] | loss: 26.02\n",
      "step: [850/1573] | loss: 119.0\n",
      "step: [900/1573] | loss: 1.226\n",
      "step: [950/1573] | loss: 4.702\n",
      "step: [1000/1573] | loss: 48.91\n",
      "step: [1050/1573] | loss: 228.5\n",
      "step: [1100/1573] | loss: 3.888\n",
      "step: [1150/1573] | loss: 3.423\n",
      "step: [1200/1573] | loss: 115.6\n",
      "step: [1250/1573] | loss: 15.09\n",
      "step: [1300/1573] | loss: 9.104\n",
      "step: [1350/1573] | loss: 22.16\n",
      "step: [1400/1573] | loss: 2.426\n",
      "step: [1450/1573] | loss: 10.63\n",
      "step: [1500/1573] | loss: 27.01\n",
      "step: [1550/1573] | loss: 11.55\n",
      "epoch loss: 71.2645\n",
      "\n",
      "===============================\n",
      "\n",
      "epoch: [3/5]\n",
      "step: [0/1573] | loss: 2.727\n",
      "step: [50/1573] | loss: 13.66\n",
      "step: [100/1573] | loss: 16.15\n",
      "step: [150/1573] | loss: 5.522\n",
      "step: [200/1573] | loss: 4.11\n",
      "step: [250/1573] | loss: 61.61\n",
      "step: [300/1573] | loss: 85.87\n",
      "step: [350/1573] | loss: 1.867\n",
      "step: [400/1573] | loss: 11.16\n",
      "step: [450/1573] | loss: 58.71\n",
      "step: [500/1573] | loss: 42.19\n",
      "step: [550/1573] | loss: 11.33\n",
      "step: [600/1573] | loss: 26.36\n",
      "step: [650/1573] | loss: 12.68\n",
      "step: [700/1573] | loss: 117.7\n",
      "step: [750/1573] | loss: 236.1\n",
      "step: [800/1573] | loss: 50.56\n",
      "step: [850/1573] | loss: 97.16\n",
      "step: [900/1573] | loss: 4.375\n",
      "step: [950/1573] | loss: 2.276\n",
      "step: [1000/1573] | loss: 385.3\n",
      "step: [1050/1573] | loss: 73.43\n",
      "step: [1100/1573] | loss: 35.61\n",
      "step: [1150/1573] | loss: 5.955\n",
      "step: [1200/1573] | loss: 11.0\n",
      "step: [1250/1573] | loss: 28.75\n",
      "step: [1300/1573] | loss: 62.7\n",
      "step: [1350/1573] | loss: 33.53\n",
      "step: [1400/1573] | loss: 12.4\n",
      "step: [1450/1573] | loss: 31.88\n",
      "step: [1500/1573] | loss: 9.294\n",
      "step: [1550/1573] | loss: 3.188\n",
      "epoch loss: 71.2994\n",
      "\n",
      "===============================\n",
      "\n",
      "epoch: [4/5]\n",
      "step: [0/1573] | loss: 463.9\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 47.54 GiB total capacity; 45.25 GiB already allocated; 33.12 MiB free; 46.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     notes_gpu \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m notes\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     19\u001b[0m     notes_X_gpu\u001b[38;5;241m.\u001b[39mappend(notes_gpu)\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_dynamic_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes_X_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes_intervals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, los)\n\u001b[1;32m     24\u001b[0m training_loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/multimodal.py:109\u001b[0m, in \u001b[0;36mMultimodalNetwork.forward\u001b[0;34m(self, packed_dynamic_X, notes_X_batch, notes_intervals_batch)\u001b[0m\n\u001b[1;32m    107\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (patient_notes, notes_interval) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(notes_X_batch, notes_intervals_batch):\n\u001b[0;32m--> 109\u001b[0m     patient_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpatient_notes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m    110\u001b[0m     weighted_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweighted_sum(embeddings\u001b[38;5;241m=\u001b[39mpatient_embeddings, interval\u001b[38;5;241m=\u001b[39mnotes_interval, decay_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_factor)\n\u001b[1;32m    111\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(weighted_sum)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1011\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1012\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1013\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1017\u001b[0m )\n\u001b[0;32m-> 1018\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    598\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    600\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    483\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 423\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    351\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 47.54 GiB total capacity; 45.25 GiB already allocated; 33.12 MiB free; 46.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "\n",
    "        loss = criterion(outputs, los)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f'step: [{step}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step == 0 and epoch == 1:\n",
    "                with open('data/losses/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open('data/losses/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open('data/losses/loss_epoch.txt', 'w') as loss_epoch_f:\n",
    "            loss_epoch_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open('data/losses/loss_epoch.txt', 'a') as loss_epoch_f:\n",
    "            loss_epoch_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print('===============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
