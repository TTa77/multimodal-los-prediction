{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Fully Frozen Bert Weights (Classification)\n",
    "- The patients here have not been separated based on whether they died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalClassifierDataset, LOSNetWeighted, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  7 18:48:59 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 37%   55C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/split/with-outliers/combined'\n",
    "\n",
    "static_train = pd.read_csv(f'{base_path}/static_train.csv')\n",
    "static_val = pd.read_csv(f'{base_path}/static_val.csv')\n",
    "static_test = pd.read_csv(f'{base_path}/static_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/notes_cleaned.csv')\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])]\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])]\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "dynamic_train = dynamic[dynamic['id'].isin(static_train['id'])].copy()\n",
    "dynamic_val = dynamic[dynamic['id'].isin(static_val['id'])].copy()\n",
    "dynamic_test = dynamic[dynamic['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28793466</td>\n",
       "      <td>4/12/29 3:35</td>\n",
       "      <td>0.076122</td>\n",
       "      <td>-0.358918</td>\n",
       "      <td>-0.907262</td>\n",
       "      <td>0.441440</td>\n",
       "      <td>0.577309</td>\n",
       "      <td>-0.713705</td>\n",
       "      <td>0.061863</td>\n",
       "      <td>0.287982</td>\n",
       "      <td>-0.640548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28164589</td>\n",
       "      <td>3/11/59 1:11</td>\n",
       "      <td>-0.745351</td>\n",
       "      <td>2.006437</td>\n",
       "      <td>0.803869</td>\n",
       "      <td>-0.096334</td>\n",
       "      <td>0.165747</td>\n",
       "      <td>0.014806</td>\n",
       "      <td>-0.704427</td>\n",
       "      <td>1.503503</td>\n",
       "      <td>-1.165692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20329785</td>\n",
       "      <td>5/12/34 19:32</td>\n",
       "      <td>-0.950719</td>\n",
       "      <td>2.734239</td>\n",
       "      <td>-0.200490</td>\n",
       "      <td>1.839650</td>\n",
       "      <td>-1.892064</td>\n",
       "      <td>-0.616570</td>\n",
       "      <td>-0.633693</td>\n",
       "      <td>-0.753893</td>\n",
       "      <td>-0.377975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24566943</td>\n",
       "      <td>6/25/55 15:45</td>\n",
       "      <td>0.076122</td>\n",
       "      <td>-0.176968</td>\n",
       "      <td>0.320289</td>\n",
       "      <td>0.333885</td>\n",
       "      <td>0.577309</td>\n",
       "      <td>0.063373</td>\n",
       "      <td>0.285855</td>\n",
       "      <td>0.461628</td>\n",
       "      <td>-0.377975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21792938</td>\n",
       "      <td>4/13/28 14:18</td>\n",
       "      <td>-0.950719</td>\n",
       "      <td>0.914735</td>\n",
       "      <td>0.357487</td>\n",
       "      <td>-1.386990</td>\n",
       "      <td>-0.794565</td>\n",
       "      <td>1.423260</td>\n",
       "      <td>-0.633693</td>\n",
       "      <td>-1.274830</td>\n",
       "      <td>1.197458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0  28793466   4/12/29 3:35  0.076122    -0.358918 -0.907262  0.441440   \n",
       "2  28164589   3/11/59 1:11 -0.745351     2.006437  0.803869 -0.096334   \n",
       "3  20329785  5/12/34 19:32 -0.950719     2.734239 -0.200490  1.839650   \n",
       "4  24566943  6/25/55 15:45  0.076122    -0.176968  0.320289  0.333885   \n",
       "5  21792938  4/13/28 14:18 -0.950719     0.914735  0.357487 -1.386990   \n",
       "\n",
       "   chloride  creatinine   glucose    sodium  potassium  \n",
       "0  0.577309   -0.713705  0.061863  0.287982  -0.640548  \n",
       "2  0.165747    0.014806 -0.704427  1.503503  -1.165692  \n",
       "3 -1.892064   -0.616570 -0.633693 -0.753893  -0.377975  \n",
       "4  0.577309    0.063373  0.285855  0.461628  -0.377975  \n",
       "5 -0.794565    1.423260 -0.633693 -1.274830   1.197458  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic_train.loc[:, features] = scaler.fit_transform(dynamic_train[features])\n",
    "dynamic_val.loc[:, features] = scaler.transform(dynamic_val[features])\n",
    "dynamic_test.loc[:, features] = scaler.transform(dynamic_test[features])  \n",
    "\n",
    "dynamic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic train preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20001305    [[-0.5399823568385593, 0.36888343933745094, 0....\n",
       "20001361    [[-0.33461420568125255, -0.17696774170196863, ...\n",
       "20003491    [[-0.12924605452394583, -0.35891813538177514, ...\n",
       "20009330    [[0.2814902477906676, -1.0867197101010013, -0....\n",
       "20009550    [[0.2814902477906676, 1.2786354077364837, 0.20...\n",
       "                                  ...                        \n",
       "29993312    [[1.9244354570491216, -0.5408685290615817, 2.4...\n",
       "29994296    [[-0.5399823568385593, -0.7228189227413881, 1....\n",
       "29996513    [[0.6922265501052811, -0.35891813538177514, 0....\n",
       "29997500    [[-0.745350507995866, 2.734238557174936, -0.53...\n",
       "29998399    [[-0.745350507995866, -0.17696774170196863, -0...\n",
       "Length: 7853, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_train = dynamic_train['id'].value_counts().to_dict()\n",
    "dynamic_train = dynamic_train.sort_values(by=['id', 'charttime'])\n",
    "dynamic_train = dynamic_train.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_train['id']).agg(list)\n",
    "\n",
    "dynamic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic val preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20008098    [[0.4868583989479744, -0.7228189227413881, -0....\n",
       "20018116    [[-0.9507186591531728, 1.2786354077364837, 0.1...\n",
       "20020590    [[-0.5399823568385593, 0.5508338330172575, 0.0...\n",
       "20032048    [[-0.33461420568125255, 0.18693304565764443, -...\n",
       "20034762    [[0.2814902477906676, -0.7228189227413881, -0....\n",
       "                                  ...                        \n",
       "29934368    [[0.07612209663336089, -0.35891813538177514, -...\n",
       "29961750    [[-1.1560868103104796, 0.004982651977837905, -...\n",
       "29970938    [[-0.33461420568125255, 0.004982651977837905, ...\n",
       "29981257    [[-0.12924605452394583, 0.004982651977837905, ...\n",
       "29981653    [[-0.5399823568385593, 0.5508338330172575, -0....\n",
       "Length: 873, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_val = dynamic_val['id'].value_counts().to_dict()\n",
    "dynamic_val = dynamic_val.sort_values(by=['id', 'charttime'])\n",
    "dynamic_val = dynamic_val.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_val['id']).agg(list)\n",
    "\n",
    "dynamic_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20003425    [[-1.5668231126250929, 1.096685014056677, -0.3...\n",
       "20015722    [[-0.9507186591531728, 0.18693304565764443, -0...\n",
       "20042079    [[-0.745350507995866, 0.36888343933745094, 0.0...\n",
       "20042619    [[-0.33461420568125255, 0.5508338330172575, -0...\n",
       "20043333    [[-0.745350507995866, -0.35891813538177514, -0...\n",
       "                                  ...                        \n",
       "29914458    [[0.8975947012625878, -0.17696774170196863, -0...\n",
       "29923363    [[-0.12924605452394583, -0.17696774170196863, ...\n",
       "29925024    [[-0.5399823568385593, -0.35891813538177514, -...\n",
       "29954601    [[0.8975947012625878, 0.36888343933745094, 0.3...\n",
       "29988601    [[0.6922265501052811, -0.17696774170196863, 0....\n",
       "Length: 970, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_test = dynamic_test['id'].value_counts().to_dict()\n",
    "dynamic_test = dynamic_test.sort_values(by=['id', 'charttime'])\n",
    "dynamic_test = dynamic_test.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_test['id']).agg(list)\n",
    "\n",
    "dynamic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])].copy()\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])].copy()\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultimodalClassifierDataset(\n",
    "    static=static_train, dynamic=dynamic_train, \n",
    "    id_lengths=id_lengths_train, notes=notes_train\n",
    "    )\n",
    "validation_data = MultimodalClassifierDataset(\n",
    "    static=static_val, dynamic=dynamic_val, \n",
    "    id_lengths=id_lengths_val, notes=notes_val\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2000, shuffle=True, collate_fn=collation)\n",
    "val_loader = DataLoader(validation_data, batch_size=400, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_features = static_train['los_icu_binned'].nunique()\n",
    "\n",
    "out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "text_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for params in text_model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "model = LOSNetWeighted(input_size=9, out_features=out_features, hidden_size=128, text_model=text_model, task='cls')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embeddings.word_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.position_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.token_type_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: pooler.dense.weight, Frozen: True\n",
      "\n",
      "Layer: pooler.dense.bias, Frozen: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.text_model.named_parameters():\n",
    "    print(f\"Layer: {name}, Frozen: {not param.requires_grad}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  7 18:49:39 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 31%   55C    P2    67W / 230W |   1573MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_path = 'losses/bert-frozen-all/with-outliers/combined_classification'\n",
    "model_save_path = 'saved-models/no-outliers/bert_frozen_all_combined_classification_best_model.pth'\n",
    "writer = SummaryWriter('tensorboard/runs/with-outliers/combined_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: [1/200]\n",
      "step: [1/4] | loss: 2.331\n",
      "step: [2/4] | loss: 2.328\n",
      "step: [3/4] | loss: 2.325\n",
      "step: [4/4] | loss: 2.322\n",
      "Training epoch loss: 2.3265\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0038\n",
      "\n",
      "validation epoch: [1/200]\n",
      "Validation epoch loss: 2.3218\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0038\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [2/200]\n",
      "step: [1/4] | loss: 2.321\n",
      "step: [2/4] | loss: 2.319\n",
      "step: [3/4] | loss: 2.316\n",
      "step: [4/4] | loss: 2.314\n",
      "Training epoch loss: 2.3175\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0063\n",
      "\n",
      "validation epoch: [2/200]\n",
      "Validation epoch loss: 2.3118\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0063\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [3/200]\n",
      "step: [1/4] | loss: 2.312\n",
      "step: [2/4] | loss: 2.31\n",
      "step: [3/4] | loss: 2.308\n",
      "step: [4/4] | loss: 2.304\n",
      "Training epoch loss: 2.3083\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0292\n",
      "\n",
      "validation epoch: [3/200]\n",
      "Validation epoch loss: 2.3016\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0292\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [4/200]\n",
      "step: [1/4] | loss: 2.302\n",
      "step: [2/4] | loss: 2.301\n",
      "step: [3/4] | loss: 2.298\n",
      "step: [4/4] | loss: 2.298\n",
      "Training epoch loss: 2.3001\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0582\n",
      "\n",
      "validation epoch: [4/200]\n",
      "Validation epoch loss: 2.2961\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0582\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [5/200]\n",
      "step: [1/4] | loss: 2.297\n",
      "step: [2/4] | loss: 2.294\n",
      "step: [3/4] | loss: 2.293\n",
      "step: [4/4] | loss: 2.289\n",
      "Training epoch loss: 2.2932\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0578\n",
      "\n",
      "validation epoch: [5/200]\n",
      "Validation epoch loss: 2.2844\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0578\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [6/200]\n",
      "step: [1/4] | loss: 2.291\n",
      "step: [2/4] | loss: 2.287\n",
      "step: [3/4] | loss: 2.286\n",
      "step: [4/4] | loss: 2.287\n",
      "Training epoch loss: 2.2878\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [6/200]\n",
      "Validation epoch loss: 2.2862\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [7/200]\n",
      "step: [1/4] | loss: 2.283\n",
      "step: [2/4] | loss: 2.284\n",
      "step: [3/4] | loss: 2.283\n",
      "step: [4/4] | loss: 2.284\n",
      "Training epoch loss: 2.2834\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [7/200]\n",
      "Validation epoch loss: 2.2804\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [8/200]\n",
      "step: [1/4] | loss: 2.281\n",
      "step: [2/4] | loss: 2.277\n",
      "step: [3/4] | loss: 2.28\n",
      "step: [4/4] | loss: 2.279\n",
      "Training epoch loss: 2.2793\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [8/200]\n",
      "Validation epoch loss: 2.2779\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [9/200]\n",
      "step: [1/4] | loss: 2.281\n",
      "step: [2/4] | loss: 2.277\n",
      "step: [3/4] | loss: 2.273\n",
      "step: [4/4] | loss: 2.273\n",
      "Training epoch loss: 2.2761\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [9/200]\n",
      "Validation epoch loss: 2.2784\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [10/200]\n",
      "step: [1/4] | loss: 2.277\n",
      "step: [2/4] | loss: 2.274\n",
      "step: [3/4] | loss: 2.269\n",
      "step: [4/4] | loss: 2.276\n",
      "Training epoch loss: 2.2738\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [10/200]\n",
      "Validation epoch loss: 2.2794\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [11/200]\n",
      "step: [1/4] | loss: 2.277\n",
      "step: [2/4] | loss: 2.268\n",
      "step: [3/4] | loss: 2.271\n",
      "step: [4/4] | loss: 2.274\n",
      "Training epoch loss: 2.2722\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [11/200]\n",
      "Validation epoch loss: 2.2751\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [12/200]\n",
      "step: [1/4] | loss: 2.275\n",
      "step: [2/4] | loss: 2.267\n",
      "step: [3/4] | loss: 2.273\n",
      "step: [4/4] | loss: 2.27\n",
      "Training epoch loss: 2.2713\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [12/200]\n",
      "Validation epoch loss: 2.2700\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [13/200]\n",
      "step: [1/4] | loss: 2.272\n",
      "step: [2/4] | loss: 2.268\n",
      "step: [3/4] | loss: 2.268\n",
      "step: [4/4] | loss: 2.277\n",
      "Training epoch loss: 2.2711\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [13/200]\n",
      "Validation epoch loss: 2.2694\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [14/200]\n",
      "step: [1/4] | loss: 2.275\n",
      "step: [2/4] | loss: 2.269\n",
      "step: [3/4] | loss: 2.267\n",
      "step: [4/4] | loss: 2.272\n",
      "Training epoch loss: 2.2709\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [14/200]\n",
      "Validation epoch loss: 2.2829\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [15/200]\n",
      "step: [1/4] | loss: 2.272\n",
      "step: [2/4] | loss: 2.268\n",
      "step: [3/4] | loss: 2.272\n",
      "step: [4/4] | loss: 2.27\n",
      "Training epoch loss: 2.2708\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [15/200]\n",
      "Validation epoch loss: 2.2669\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [16/200]\n",
      "step: [1/4] | loss: 2.268\n",
      "step: [2/4] | loss: 2.262\n",
      "step: [3/4] | loss: 2.274\n",
      "step: [4/4] | loss: 2.279\n",
      "Training epoch loss: 2.2710\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [16/200]\n",
      "Validation epoch loss: 2.2703\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [17/200]\n",
      "step: [1/4] | loss: 2.276\n",
      "step: [2/4] | loss: 2.268\n",
      "step: [3/4] | loss: 2.269\n",
      "step: [4/4] | loss: 2.269\n",
      "Training epoch loss: 2.2708\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [17/200]\n",
      "Validation epoch loss: 2.2663\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [18/200]\n",
      "step: [1/4] | loss: 2.267\n",
      "step: [2/4] | loss: 2.264\n",
      "step: [3/4] | loss: 2.273\n",
      "step: [4/4] | loss: 2.279\n",
      "Training epoch loss: 2.2709\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [18/200]\n",
      "Validation epoch loss: 2.2722\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [19/200]\n",
      "step: [1/4] | loss: 2.283\n",
      "step: [2/4] | loss: 2.271\n",
      "step: [3/4] | loss: 2.257\n",
      "step: [4/4] | loss: 2.273\n",
      "Training epoch loss: 2.2709\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [19/200]\n",
      "Validation epoch loss: 2.2586\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [20/200]\n",
      "step: [1/4] | loss: 2.277\n",
      "step: [2/4] | loss: 2.273\n",
      "step: [3/4] | loss: 2.26\n",
      "step: [4/4] | loss: 2.272\n",
      "Training epoch loss: 2.2707\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [20/200]\n",
      "Validation epoch loss: 2.2610\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [21/200]\n",
      "step: [1/4] | loss: 2.268\n",
      "step: [2/4] | loss: 2.271\n",
      "step: [3/4] | loss: 2.273\n",
      "step: [4/4] | loss: 2.27\n",
      "Training epoch loss: 2.2707\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [21/200]\n",
      "Validation epoch loss: 2.2743\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [22/200]\n",
      "step: [1/4] | loss: 2.268\n",
      "step: [2/4] | loss: 2.267\n",
      "step: [3/4] | loss: 2.271\n",
      "step: [4/4] | loss: 2.276\n",
      "Training epoch loss: 2.2708\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [22/200]\n",
      "Validation epoch loss: 2.2696\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [23/200]\n",
      "step: [1/4] | loss: 2.27\n",
      "step: [2/4] | loss: 2.273\n",
      "step: [3/4] | loss: 2.279\n",
      "step: [4/4] | loss: 2.261\n",
      "Training epoch loss: 2.2705\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [23/200]\n",
      "Validation epoch loss: 2.2638\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [24/200]\n",
      "step: [1/4] | loss: 2.277\n",
      "step: [2/4] | loss: 2.268\n",
      "step: [3/4] | loss: 2.268\n",
      "step: [4/4] | loss: 2.269\n",
      "Training epoch loss: 2.2707\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [24/200]\n",
      "Validation epoch loss: 2.2784\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [25/200]\n",
      "step: [1/4] | loss: 2.27\n",
      "step: [2/4] | loss: 2.276\n",
      "step: [3/4] | loss: 2.262\n",
      "step: [4/4] | loss: 2.273\n",
      "Training epoch loss: 2.2706\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [25/200]\n",
      "Validation epoch loss: 2.2691\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [26/200]\n",
      "step: [1/4] | loss: 2.267\n",
      "step: [2/4] | loss: 2.273\n",
      "step: [3/4] | loss: 2.27\n",
      "step: [4/4] | loss: 2.273\n",
      "Training epoch loss: 2.2706\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [26/200]\n",
      "Validation epoch loss: 2.2527\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [27/200]\n",
      "step: [1/4] | loss: 2.264\n",
      "step: [2/4] | loss: 2.272\n",
      "step: [3/4] | loss: 2.277\n",
      "step: [4/4] | loss: 2.269\n",
      "Training epoch loss: 2.2706\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [27/200]\n",
      "Validation epoch loss: 2.2663\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [28/200]\n",
      "step: [1/4] | loss: 2.265\n",
      "step: [2/4] | loss: 2.269\n",
      "step: [3/4] | loss: 2.27\n",
      "step: [4/4] | loss: 2.279\n",
      "Training epoch loss: 2.2707\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [28/200]\n",
      "Validation epoch loss: 2.2798\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [29/200]\n",
      "step: [1/4] | loss: 2.266\n",
      "step: [2/4] | loss: 2.259\n",
      "step: [3/4] | loss: 2.277\n",
      "step: [4/4] | loss: 2.281\n",
      "Training epoch loss: 2.2707\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [29/200]\n",
      "Validation epoch loss: 2.2638\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [30/200]\n",
      "step: [1/4] | loss: 2.282\n",
      "step: [2/4] | loss: 2.27\n",
      "step: [3/4] | loss: 2.262\n",
      "step: [4/4] | loss: 2.268\n",
      "Training epoch loss: 2.2705\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [30/200]\n",
      "Validation epoch loss: 2.2640\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [31/200]\n",
      "step: [1/4] | loss: 2.281\n",
      "step: [2/4] | loss: 2.264\n",
      "step: [3/4] | loss: 2.268\n",
      "step: [4/4] | loss: 2.269\n",
      "Training epoch loss: 2.2705\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [31/200]\n",
      "Validation epoch loss: 2.2741\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [32/200]\n",
      "step: [1/4] | loss: 2.272\n",
      "step: [2/4] | loss: 2.269\n",
      "step: [3/4] | loss: 2.278\n",
      "step: [4/4] | loss: 2.263\n",
      "Training epoch loss: 2.2704\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [32/200]\n",
      "Validation epoch loss: 2.2749\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [33/200]\n",
      "step: [1/4] | loss: 2.265\n",
      "step: [2/4] | loss: 2.271\n",
      "step: [3/4] | loss: 2.273\n",
      "step: [4/4] | loss: 2.273\n",
      "Training epoch loss: 2.2705\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [33/200]\n",
      "Validation epoch loss: 2.2798\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [34/200]\n",
      "step: [1/4] | loss: 2.268\n",
      "step: [2/4] | loss: 2.269\n",
      "step: [3/4] | loss: 2.277\n",
      "step: [4/4] | loss: 2.269\n",
      "Training epoch loss: 2.2705\n",
      "\n",
      "Training Weighted F1 epoch score: 0.0568\n",
      "\n",
      "validation epoch: [34/200]\n",
      "Validation epoch loss: 2.2745\n",
      "\n",
      "Validation Weighted F1 epoch score: 0.0568\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [35/200]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     notes_gpu \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m notes\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     26\u001b[0m     notes_X_gpu\u001b[38;5;241m.\u001b[39mappend(notes_gpu)\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_dynamic_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes_X_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes_intervals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(los, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/multimodal.py:157\u001b[0m, in \u001b[0;36mLOSNetWeighted.forward\u001b[0;34m(self, packed_dynamic_X_batch, notes_X_batch, notes_intervals_batch)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (patient_notes, notes_interval) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(notes_X_batch, notes_intervals_batch):\n\u001b[1;32m    156\u001b[0m     patient_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpatient_notes)\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[0;32m--> 157\u001b[0m     weighted_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweighted_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatient_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnotes_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecay_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(weighted_sum)\n\u001b[1;32m    160\u001b[0m zt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(embeddings)\n",
      "File \u001b[0;32m/notebooks/multimodal.py:145\u001b[0m, in \u001b[0;36mLOSNetWeighted.weighted_sum\u001b[0;34m(self, embeddings, interval, decay_factor)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweighted_sum\u001b[39m(\u001b[38;5;28mself\u001b[39m, embeddings, interval, decay_factor):\n\u001b[1;32m    144\u001b[0m     device \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 145\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay_factor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     weighted_sum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(weights, embeddings)\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weighted_sum\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "train_weighted_f1_scores = []\n",
    "val_weighted_f1_scores = []\n",
    "patience = 10\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range (1, epochs+1):\n",
    "\n",
    "    print(f'training epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        true_labels = torch.argmax(los, dim=1)\n",
    "\n",
    "        loss = criterion(outputs, true_labels)\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + step)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        all_true_labels.extend(true_labels.cpu().numpy())\n",
    "        all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "        if step % max(1, round(len(train_loader) * 0.1)) == 0:\n",
    "            print(f'step: [{step+1}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step+1 == 1 and epoch == 1:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    writer.add_scalar('Loss/train_avg', avg_training_loss_epoch.item(), epoch)\n",
    "\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'Training epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    train_weighted_f1_score = f1_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "    train_weighted_f1_scores.append(round(train_weighted_f1_score, 4))\n",
    "    print(f'Training Weighted F1 epoch score: {train_weighted_f1_score:.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'w') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'a') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print(f'validation epoch: [{epoch}/{epochs}]')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss_epoch = 0\n",
    "        val_all_true_labels = []\n",
    "        val_all_predicted_labels = []\n",
    "\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            packed_dynamic_X_val, notes_X_val, notes_intervals_val, los_val = val_batch\n",
    "\n",
    "            packed_dynamic_X_val = packed_dynamic_X_val.to(device)\n",
    "            los_val = los_val.to(device)\n",
    "\n",
    "            notes_X_val_gpu = []\n",
    "            for notes in notes_X_val:\n",
    "                notes_val_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "                notes_X_val_gpu.append(notes_val_gpu)\n",
    "\n",
    "            val_outputs = model(packed_dynamic_X_val, notes_X_val_gpu, notes_intervals_val)\n",
    "\n",
    "            val_predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            val_true_labels = torch.argmax(los_val, dim=1)\n",
    "\n",
    "            val_loss = criterion(val_outputs, val_true_labels)\n",
    "            writer.add_scalar('Loss/val', val_loss.item(), epoch * len(val_loader) + val_step)\n",
    "            validation_loss_epoch += val_loss\n",
    "\n",
    "            val_all_true_labels.extend(true_labels.cpu().numpy())\n",
    "            val_all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "        writer.add_scalar('Loss/val_avg', avg_validation_loss.item(), epoch)\n",
    "        print(f'Validation epoch loss: {avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        val_weighted_f1_score = f1_score(val_all_true_labels, val_all_predicted_labels, average='weighted')\n",
    "        val_weighted_f1_scores.append(round(train_weighted_f1_score, 4))\n",
    "        print(f'Validation Weighted F1 epoch score: {train_weighted_f1_score:.4f}\\n')\n",
    "        \n",
    "        if len(validation_loss) == 0 or (avg_validation_loss.item() < min(validation_loss)):\n",
    "            stagnation = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'new minimum validation loss')\n",
    "            print(f'model saved\\n')\n",
    "\n",
    "\n",
    "        else:\n",
    "            stagnation += 1\n",
    "\n",
    "        validation_loss.append(avg_validation_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'w') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        else:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'a') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        if stagnation >= patience:\n",
    "            print(f'No improvement over {patience} epochs')\n",
    "            print('Early stopping\\n')\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('===============================\\n')\n",
    "\n",
    "writer.close()\n",
    "print(f'min training loss: {min(training_loss):.4f}')\n",
    "print(f'min validation loss: {min(validation_loss):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
