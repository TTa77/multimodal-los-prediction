{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Fully Frozen Bert Weights (Regression)\n",
    "- The patients here have not been separated based on whether they died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalDataset, LOSNetWeighted, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  6 19:59:34 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   37C    P8    21W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/regression/no-outliers/combined'\n",
    "\n",
    "static_train = pd.read_csv(f'{base_path}/static_train.csv')\n",
    "static_val = pd.read_csv(f'{base_path}/static_val.csv')\n",
    "static_test = pd.read_csv(f'{base_path}/static_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2311, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/notes_with_interval.csv')\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])]\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])]\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "dynamic_train = dynamic[dynamic['id'].isin(static_train['id'])].copy()\n",
    "dynamic_val = dynamic[dynamic['id'].isin(static_val['id'])].copy()\n",
    "dynamic_test = dynamic[dynamic['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>-0.712744</td>\n",
       "      <td>-0.319670</td>\n",
       "      <td>-1.032452</td>\n",
       "      <td>-0.650622</td>\n",
       "      <td>1.438007</td>\n",
       "      <td>-0.734532</td>\n",
       "      <td>-0.565091</td>\n",
       "      <td>0.725645</td>\n",
       "      <td>-0.900854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21792938</td>\n",
       "      <td>4/13/28 14:18</td>\n",
       "      <td>-0.931146</td>\n",
       "      <td>0.794023</td>\n",
       "      <td>0.371756</td>\n",
       "      <td>-1.520537</td>\n",
       "      <td>-0.679054</td>\n",
       "      <td>1.532324</td>\n",
       "      <td>-0.588363</td>\n",
       "      <td>-1.249342</td>\n",
       "      <td>1.369643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28398464</td>\n",
       "      <td>12/9/34 8:10</td>\n",
       "      <td>0.597668</td>\n",
       "      <td>-0.505286</td>\n",
       "      <td>-0.463178</td>\n",
       "      <td>1.959125</td>\n",
       "      <td>-0.114505</td>\n",
       "      <td>-0.683012</td>\n",
       "      <td>-0.611635</td>\n",
       "      <td>-0.351621</td>\n",
       "      <td>-0.333230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21525925</td>\n",
       "      <td>2/10/45 5:40</td>\n",
       "      <td>-0.275940</td>\n",
       "      <td>-0.134054</td>\n",
       "      <td>-1.146306</td>\n",
       "      <td>-0.541882</td>\n",
       "      <td>0.591182</td>\n",
       "      <td>-0.837571</td>\n",
       "      <td>-0.378918</td>\n",
       "      <td>0.187012</td>\n",
       "      <td>-0.758948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22576776</td>\n",
       "      <td>1/13/84 7:30</td>\n",
       "      <td>1.252874</td>\n",
       "      <td>-0.505286</td>\n",
       "      <td>-1.146306</td>\n",
       "      <td>-0.106925</td>\n",
       "      <td>0.167770</td>\n",
       "      <td>-0.631493</td>\n",
       "      <td>-0.565091</td>\n",
       "      <td>0.546101</td>\n",
       "      <td>-0.617042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0  26115624    9/7/50 0:22 -0.712744    -0.319670 -1.032452 -0.650622   \n",
       "1  21792938  4/13/28 14:18 -0.931146     0.794023  0.371756 -1.520537   \n",
       "2  28398464   12/9/34 8:10  0.597668    -0.505286 -0.463178  1.959125   \n",
       "6  21525925   2/10/45 5:40 -0.275940    -0.134054 -1.146306 -0.541882   \n",
       "7  22576776   1/13/84 7:30  1.252874    -0.505286 -1.146306 -0.106925   \n",
       "\n",
       "   chloride  creatinine   glucose    sodium  potassium  \n",
       "0  1.438007   -0.734532 -0.565091  0.725645  -0.900854  \n",
       "1 -0.679054    1.532324 -0.588363 -1.249342   1.369643  \n",
       "2 -0.114505   -0.683012 -0.611635 -0.351621  -0.333230  \n",
       "6  0.591182   -0.837571 -0.378918  0.187012  -0.758948  \n",
       "7  0.167770   -0.631493 -0.565091  0.546101  -0.617042  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic_train.loc[:, features] = scaler.fit_transform(dynamic_train[features])\n",
    "dynamic_val.loc[:, features] = scaler.transform(dynamic_val[features])\n",
    "dynamic_test.loc[:, features] = scaler.transform(dynamic_test[features])  \n",
    "\n",
    "dynamic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic train cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20008098    [[0.5976678649580875, -0.8765166023203315, -0....\n",
       "20013244    [[-0.49434217878227354, 0.6084077288686798, -0...\n",
       "20015730    [[-0.931146196278418, 0.237176646071427, -0.72...\n",
       "20020562    [[-0.931146196278418, 0.051561104672800596, -0...\n",
       "20022095    [[4.528904022423387, -3.2895186405024748, -0.7...\n",
       "                                  ...                        \n",
       "29988601    [[0.8160698737061598, -0.3196699781244522, 0.5...\n",
       "29990184    [[-0.0575381612861291, 1.9077165186590646, 0.2...\n",
       "29990494    [[0.8160698737061598, -0.690901060921705, -0.4...\n",
       "29994296    [[-0.49434217878227354, -0.8765166023203315, 1...\n",
       "29997500    [[-0.7127441875303457, 2.65017868425357, -0.53...\n",
       "Length: 2311, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_train = dynamic_train['id'].value_counts().to_dict()\n",
    "dynamic_train = dynamic_train.sort_values(by=['id', 'charttime'])\n",
    "dynamic_train = dynamic_train.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_train['id']).agg(list)\n",
    "\n",
    "dynamic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic val cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20042619    [[-0.27594017003420135, 0.4227921874700534, -0...\n",
       "20098037    [[-0.49434217878227354, 0.051561104672800596, ...\n",
       "20111879    [[-0.931146196278418, 0.237176646071427, 0.144...\n",
       "20197360    [[-0.0575381612861291, -0.13405443672582582, -...\n",
       "20226142    [[-0.49434217878227354, 1.165254353064559, -0....\n",
       "                                  ...                        \n",
       "29669406    [[0.16086384746194313, 0.237176646071427, -0.9...\n",
       "29742461    [[0.5976678649580875, -1.4333632265162106, 0.4...\n",
       "29801793    [[-0.7127441875303457, -0.8765166023203315, 0....\n",
       "29841097    [[-0.931146196278418, 0.7940232702673062, -0.7...\n",
       "29862044    [[-1.804754231270707, 0.9796388116659326, -0.6...\n",
       "Length: 257, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_val = dynamic_val['id'].value_counts().to_dict()\n",
    "dynamic_val = dynamic_val.sort_values(by=['id', 'charttime'])\n",
    "dynamic_val = dynamic_val.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_val['id']).agg(list)\n",
    "\n",
    "dynamic_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic test cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20021110    [[0.16086384746194313, 0.6084077288686798, -0....\n",
       "20030660    [[-0.931146196278418, -0.13405443672582582, -1...\n",
       "20114766    [[-0.0575381612861291, -0.690901060921705, 0.4...\n",
       "20128881    [[-0.0575381612861291, -0.3196699781244522, -0...\n",
       "20134597    [[-0.0575381612861291, -0.5052855195230787, -0...\n",
       "                                  ...                        \n",
       "29865517    [[2.5632859436907376, -2.1758253921107165, 0.2...\n",
       "29891413    [[-1.804754231270707, -0.13405443672582582, -0...\n",
       "29907670    [[-0.7127441875303457, 0.051561104672800596, -...\n",
       "29923363    [[-0.0575381612861291, -0.3196699781244522, -1...\n",
       "29970938    [[-0.27594017003420135, -0.13405443672582582, ...\n",
       "Length: 286, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_test = dynamic_test['id'].value_counts().to_dict()\n",
    "dynamic_test = dynamic_test.sort_values(by=['id', 'charttime'])\n",
    "dynamic_test = dynamic_test.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_test['id']).agg(list)\n",
    "\n",
    "dynamic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])].copy()\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])].copy()\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216637b5a10a463691692ca2156d968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520f9a3aa4a84858bff969f570c9f45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = MultimodalDataset(static=static_train, dynamic=dynamic_train, id_lengths=id_lengths_train, notes=notes_train)\n",
    "validation_data = MultimodalDataset(static=static_val, dynamic=dynamic_val, id_lengths=id_lengths_val, notes=notes_val)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=collation)\n",
    "val_loader = DataLoader(validation_data, batch_size=2, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037078b24fca42b589c0ef8113a1c450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "text_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for layer in text_model.encoder.layer[:11]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = LOSNetWeighted(input_size=9, out_features=1, hidden_size=128, text_model=text_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: time_series_model.weight_ih_l0, Frozen: False\n",
      "\n",
      "Layer: time_series_model.weight_hh_l0, Frozen: False\n",
      "\n",
      "Layer: time_series_model.bias_ih_l0, Frozen: False\n",
      "\n",
      "Layer: time_series_model.bias_hh_l0, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.word_embeddings.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.position_embeddings.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.token_type_embeddings.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.LayerNorm.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.LayerNorm.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.query.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.query.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.key.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.key.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.value.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.value.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.dense.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.LayerNorm.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.LayerNorm.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.intermediate.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.intermediate.dense.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.dense.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.LayerNorm.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.LayerNorm.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.pooler.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.pooler.dense.bias, Frozen: False\n",
      "\n",
      "Layer: fc.0.weight, Frozen: False\n",
      "\n",
      "Layer: fc.0.bias, Frozen: False\n",
      "\n",
      "Layer: fc.1.weight, Frozen: False\n",
      "\n",
      "Layer: fc.1.bias, Frozen: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Frozen: {not param.requires_grad}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, predicted, actual):\n",
    "        return torch.sqrt(self.mse(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  6 20:00:16 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   41C    P2    72W / 300W |   1641MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_path = 'losses/bert-frozen-10/no-outliers/combined_regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: [1/200]\n",
      "step: [1/1156] | loss: 0.9487\n",
      "step: [117/1156] | loss: 1.648\n",
      "step: [233/1156] | loss: 1.871\n",
      "step: [349/1156] | loss: 1.533\n",
      "step: [465/1156] | loss: 5.734\n",
      "step: [581/1156] | loss: 1.33\n",
      "step: [697/1156] | loss: 0.8492\n",
      "step: [813/1156] | loss: 0.3162\n",
      "step: [929/1156] | loss: 1.537\n",
      "step: [1045/1156] | loss: 3.084\n",
      "Training epoch loss: 2.1998\n",
      "\n",
      "validation epoch: [1/200]\n",
      "Validation epoch loss: 2.2127\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [2/200]\n",
      "step: [1/1156] | loss: 1.482\n",
      "step: [117/1156] | loss: 2.25\n",
      "step: [233/1156] | loss: 0.5952\n",
      "step: [349/1156] | loss: 2.205\n",
      "step: [465/1156] | loss: 4.775\n",
      "step: [581/1156] | loss: 1.462\n",
      "step: [697/1156] | loss: 4.531\n",
      "step: [813/1156] | loss: 2.073\n",
      "step: [929/1156] | loss: 1.228\n",
      "step: [1045/1156] | loss: 6.237\n",
      "Training epoch loss: 2.1548\n",
      "\n",
      "validation epoch: [2/200]\n",
      "Validation epoch loss: 2.2280\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [3/200]\n",
      "step: [1/1156] | loss: 6.023\n",
      "step: [117/1156] | loss: 1.612\n",
      "step: [233/1156] | loss: 0.2578\n",
      "step: [349/1156] | loss: 1.423\n",
      "step: [465/1156] | loss: 1.225\n",
      "step: [581/1156] | loss: 1.845\n",
      "step: [697/1156] | loss: 4.793\n",
      "step: [813/1156] | loss: 2.379\n",
      "step: [929/1156] | loss: 1.187\n",
      "step: [1045/1156] | loss: 0.6944\n",
      "Training epoch loss: 2.1426\n",
      "\n",
      "validation epoch: [3/200]\n",
      "Validation epoch loss: 2.2493\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [4/200]\n",
      "step: [1/1156] | loss: 1.643\n",
      "step: [117/1156] | loss: 0.8739\n",
      "step: [233/1156] | loss: 1.362\n",
      "step: [349/1156] | loss: 1.135\n",
      "step: [465/1156] | loss: 1.038\n",
      "step: [581/1156] | loss: 1.256\n",
      "step: [697/1156] | loss: 1.972\n",
      "step: [813/1156] | loss: 5.139\n",
      "step: [929/1156] | loss: 1.528\n",
      "step: [1045/1156] | loss: 1.536\n",
      "Training epoch loss: 2.1181\n",
      "\n",
      "validation epoch: [4/200]\n",
      "Validation epoch loss: 2.2275\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [5/200]\n",
      "step: [1/1156] | loss: 6.207\n",
      "step: [117/1156] | loss: 1.896\n",
      "step: [233/1156] | loss: 1.08\n",
      "step: [349/1156] | loss: 1.427\n",
      "step: [465/1156] | loss: 1.391\n",
      "step: [581/1156] | loss: 3.608\n",
      "step: [697/1156] | loss: 0.8969\n",
      "step: [813/1156] | loss: 1.307\n",
      "step: [929/1156] | loss: 1.547\n",
      "step: [1045/1156] | loss: 1.935\n",
      "Training epoch loss: 2.0727\n",
      "\n",
      "validation epoch: [5/200]\n",
      "Validation epoch loss: 2.2344\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [6/200]\n",
      "step: [1/1156] | loss: 0.5744\n",
      "step: [117/1156] | loss: 5.366\n",
      "step: [233/1156] | loss: 1.847\n",
      "step: [349/1156] | loss: 0.817\n",
      "step: [465/1156] | loss: 0.5815\n",
      "step: [581/1156] | loss: 1.101\n",
      "step: [697/1156] | loss: 3.336\n",
      "step: [813/1156] | loss: 1.106\n",
      "step: [929/1156] | loss: 1.25\n",
      "step: [1045/1156] | loss: 1.206\n",
      "Training epoch loss: 2.0542\n",
      "\n",
      "validation epoch: [6/200]\n",
      "Validation epoch loss: 2.2726\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [7/200]\n",
      "step: [1/1156] | loss: 0.1419\n",
      "step: [117/1156] | loss: 6.611\n",
      "step: [233/1156] | loss: 0.6269\n",
      "step: [349/1156] | loss: 0.2254\n",
      "step: [465/1156] | loss: 1.044\n",
      "step: [581/1156] | loss: 0.853\n",
      "step: [697/1156] | loss: 3.162\n",
      "step: [813/1156] | loss: 4.12\n",
      "step: [929/1156] | loss: 0.04953\n",
      "step: [1045/1156] | loss: 3.346\n",
      "Training epoch loss: 2.0149\n",
      "\n",
      "validation epoch: [7/200]\n",
      "Validation epoch loss: 2.2317\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [8/200]\n",
      "step: [1/1156] | loss: 0.6321\n",
      "step: [117/1156] | loss: 0.6364\n",
      "step: [233/1156] | loss: 0.4635\n",
      "step: [349/1156] | loss: 1.419\n",
      "step: [465/1156] | loss: 2.481\n",
      "step: [581/1156] | loss: 1.055\n",
      "step: [697/1156] | loss: 1.671\n",
      "step: [813/1156] | loss: 1.007\n",
      "step: [929/1156] | loss: 0.8589\n",
      "step: [1045/1156] | loss: 0.8896\n",
      "Training epoch loss: 1.9383\n",
      "\n",
      "validation epoch: [8/200]\n",
      "Validation epoch loss: 2.2493\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [9/200]\n",
      "step: [1/1156] | loss: 0.5411\n",
      "step: [117/1156] | loss: 0.474\n",
      "step: [233/1156] | loss: 0.806\n",
      "step: [349/1156] | loss: 1.622\n",
      "step: [465/1156] | loss: 1.655\n",
      "step: [581/1156] | loss: 1.63\n",
      "step: [697/1156] | loss: 1.81\n",
      "step: [813/1156] | loss: 1.647\n",
      "step: [929/1156] | loss: 0.8138\n",
      "step: [1045/1156] | loss: 0.5224\n",
      "Training epoch loss: 1.9060\n",
      "\n",
      "validation epoch: [9/200]\n",
      "Validation epoch loss: 2.2757\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [10/200]\n",
      "step: [1/1156] | loss: 0.5864\n",
      "step: [117/1156] | loss: 0.879\n",
      "step: [233/1156] | loss: 0.8054\n",
      "step: [349/1156] | loss: 0.6851\n",
      "step: [465/1156] | loss: 2.346\n",
      "step: [581/1156] | loss: 3.57\n",
      "step: [697/1156] | loss: 0.5818\n",
      "step: [813/1156] | loss: 1.963\n",
      "step: [929/1156] | loss: 1.158\n",
      "step: [1045/1156] | loss: 1.475\n",
      "Training epoch loss: 1.8179\n",
      "\n",
      "validation epoch: [10/200]\n",
      "Validation epoch loss: 2.2474\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [11/200]\n",
      "step: [1/1156] | loss: 2.03\n",
      "step: [117/1156] | loss: 2.777\n",
      "step: [233/1156] | loss: 3.227\n",
      "step: [349/1156] | loss: 2.72\n",
      "step: [465/1156] | loss: 2.688\n",
      "step: [581/1156] | loss: 0.4848\n",
      "step: [697/1156] | loss: 1.915\n",
      "step: [813/1156] | loss: 0.7513\n",
      "step: [929/1156] | loss: 0.9919\n",
      "step: [1045/1156] | loss: 1.591\n",
      "Training epoch loss: 1.7862\n",
      "\n",
      "validation epoch: [11/200]\n",
      "Validation epoch loss: 2.3019\n",
      "\n",
      "No improvement over 10 epochs\n",
      "Early stopping\n",
      "min epoch loss: 1.7862317562103271\n",
      "min validation loss: 2.212693452835083\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "patience = 10\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'training epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "\n",
    "        loss = criterion(outputs, los)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % max(1, round(len(train_loader) * 0.1)) == 0:\n",
    "            print(f'step: [{step+1}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step+1 == 1 and epoch == 1:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'Training epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'w') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'a') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print(f'validation epoch: [{epoch}/{epochs}]')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss_epoch = 0\n",
    "\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            packed_dynamic_X_val, notes_X_val, notes_intervals_val, los_val = val_batch\n",
    "\n",
    "            packed_dynamic_X_val = packed_dynamic_X_val.to(device)\n",
    "            los_val = los_val.to(device)\n",
    "\n",
    "            notes_X_val_gpu = []\n",
    "            for notes in notes_X_val:\n",
    "                notes_val_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "                notes_X_val_gpu.append(notes_val_gpu)\n",
    "\n",
    "            val_outputs = model(packed_dynamic_X_val, notes_X_val_gpu, notes_intervals_val)\n",
    "            val_loss = criterion(val_outputs, los_val)\n",
    "            validation_loss_epoch += val_loss\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "        print(f'Validation epoch loss: {avg_validation_loss.item():.4f}\\n')\n",
    "        \n",
    "        if len(validation_loss) == 0 or (avg_validation_loss.item() < min(validation_loss)):\n",
    "            stagnation = 0\n",
    "            torch.save(model.state_dict(), 'saved-models/no-outliers/bert_frozen_10_regression_best_model.pth')\n",
    "            print(f'new minimum validation loss')\n",
    "            print(f'model saved\\n')\n",
    "\n",
    "        else:\n",
    "            stagnation += 1\n",
    "\n",
    "        validation_loss.append(avg_validation_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'w') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        else:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'a') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        if stagnation >= patience:\n",
    "            print(f'No improvement over {patience} epochs')\n",
    "            print('Early stopping\\n')\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('===============================\\n')\n",
    "    \n",
    "print(f'min training loss: {min(training_loss)}')\n",
    "print(f'min validation loss: {min(validation_loss)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
