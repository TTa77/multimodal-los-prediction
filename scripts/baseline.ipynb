{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Fully Frozen Bert Weights (Regression)\n",
    "- The patients here have not been separated based on whether they died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalDataset, LOSNetWeighted, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  7 09:19:01 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   47C    P8    23W / 300W |   1641MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/regression/with-outliers/combined'\n",
    "\n",
    "static_train = pd.read_csv(f'{base_path}/static_train.csv')\n",
    "static_val = pd.read_csv(f'{base_path}/static_val.csv')\n",
    "static_test = pd.read_csv(f'{base_path}/static_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7853, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/notes_cleaned.csv')\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])]\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])]\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "dynamic_train = dynamic[dynamic['id'].isin(static_train['id'])].copy()\n",
    "dynamic_val = dynamic[dynamic['id'].isin(static_val['id'])].copy()\n",
    "dynamic_test = dynamic[dynamic['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28793466</td>\n",
       "      <td>4/12/29 3:35</td>\n",
       "      <td>0.071185</td>\n",
       "      <td>-0.349005</td>\n",
       "      <td>-0.906293</td>\n",
       "      <td>0.416715</td>\n",
       "      <td>0.575845</td>\n",
       "      <td>-0.712386</td>\n",
       "      <td>0.059663</td>\n",
       "      <td>0.297403</td>\n",
       "      <td>-0.639959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>-0.751069</td>\n",
       "      <td>-0.167053</td>\n",
       "      <td>-1.018477</td>\n",
       "      <td>-0.522606</td>\n",
       "      <td>1.260931</td>\n",
       "      <td>-0.712386</td>\n",
       "      <td>-0.606627</td>\n",
       "      <td>0.647561</td>\n",
       "      <td>-0.900545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20329785</td>\n",
       "      <td>5/12/34 19:32</td>\n",
       "      <td>-0.956633</td>\n",
       "      <td>2.744188</td>\n",
       "      <td>-0.195790</td>\n",
       "      <td>1.773512</td>\n",
       "      <td>-1.890467</td>\n",
       "      <td>-0.617347</td>\n",
       "      <td>-0.630005</td>\n",
       "      <td>-0.753070</td>\n",
       "      <td>-0.379374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24566943</td>\n",
       "      <td>6/25/55 15:45</td>\n",
       "      <td>0.071185</td>\n",
       "      <td>-0.167053</td>\n",
       "      <td>0.327738</td>\n",
       "      <td>0.312346</td>\n",
       "      <td>0.575845</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>0.281760</td>\n",
       "      <td>0.472482</td>\n",
       "      <td>-0.379374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21792938</td>\n",
       "      <td>4/13/28 14:18</td>\n",
       "      <td>-0.956633</td>\n",
       "      <td>0.924662</td>\n",
       "      <td>0.365133</td>\n",
       "      <td>-1.357559</td>\n",
       "      <td>-0.794329</td>\n",
       "      <td>1.378482</td>\n",
       "      <td>-0.630005</td>\n",
       "      <td>-1.278306</td>\n",
       "      <td>1.184140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0  28793466   4/12/29 3:35  0.071185    -0.349005 -0.906293  0.416715   \n",
       "1  26115624    9/7/50 0:22 -0.751069    -0.167053 -1.018477 -0.522606   \n",
       "3  20329785  5/12/34 19:32 -0.956633     2.744188 -0.195790  1.773512   \n",
       "4  24566943  6/25/55 15:45  0.071185    -0.167053  0.327738  0.312346   \n",
       "5  21792938  4/13/28 14:18 -0.956633     0.924662  0.365133 -1.357559   \n",
       "\n",
       "   chloride  creatinine   glucose    sodium  potassium  \n",
       "0  0.575845   -0.712386  0.059663  0.297403  -0.639959  \n",
       "1  1.260931   -0.712386 -0.606627  0.647561  -0.900545  \n",
       "3 -1.890467   -0.617347 -0.630005 -0.753070  -0.379374  \n",
       "4  0.575845    0.047929  0.281760  0.472482  -0.379374  \n",
       "5 -0.794329    1.378482 -0.630005 -1.278306   1.184140  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic_train.loc[:, features] = scaler.fit_transform(dynamic_train[features])\n",
    "dynamic_val.loc[:, features] = scaler.transform(dynamic_val[features])\n",
    "dynamic_test.loc[:, features] = scaler.transform(dynamic_test[features])  \n",
    "\n",
    "dynamic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic train preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20001305    [[-0.545505639426621, 0.37880475678178827, 0.4...\n",
       "20001361    [[-0.33994208065368914, -0.16705291664414468, ...\n",
       "20003425    [[-1.5733234332912807, 1.1066149880163654, -0....\n",
       "20008098    [[0.48231215443803854, -0.7129105900700776, -0...\n",
       "20009330    [[0.2767485956651066, -1.0768157056873662, -0....\n",
       "                                  ...                        \n",
       "29991038    [[0.48231215443803854, -0.7129105900700776, 1....\n",
       "29991539    [[0.48231215443803854, 1.1066149880163654, 0.3...\n",
       "29993312    [[1.921257065848562, -0.5309580322614332, 2.45...\n",
       "29996513    [[0.6878757132109704, -0.34900547445278896, 0....\n",
       "29998399    [[-0.751069198199553, -0.16705291664414468, -0...\n",
       "Length: 7853, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_train = dynamic_train['id'].value_counts().to_dict()\n",
    "dynamic_train = dynamic_train.sort_values(by=['id', 'charttime'])\n",
    "dynamic_train = dynamic_train.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_train['id']).agg(list)\n",
    "\n",
    "dynamic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic val preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20015730    [[-0.9566327569724848, 0.37880475678178827, -0...\n",
       "20032048    [[-0.33994208065368914, 0.19685219897314393, -...\n",
       "20048588    [[0.6878757132109704, -0.16705291664414468, -0...\n",
       "20048978    [[0.8934392719839024, -0.7129105900700776, 1.1...\n",
       "20055299    [[0.2767485956651066, 0.19685219897314393, 0.5...\n",
       "                                  ...                        \n",
       "29925024    [[-0.545505639426621, -0.34900547445278896, -0...\n",
       "29926463    [[0.8934392719839024, -1.6226733791132992, -0....\n",
       "29953754    [[0.8934392719839024, -0.34900547445278896, -0...\n",
       "29989089    [[-0.751069198199553, 0.7427098723990768, -1.1...\n",
       "29990494    [[0.6878757132109704, -0.5309580322614332, -0....\n",
       "Length: 873, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_val = dynamic_val['id'].value_counts().to_dict()\n",
    "dynamic_val = dynamic_val.sort_values(by=['id', 'charttime'])\n",
    "dynamic_val = dynamic_val.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_val['id']).agg(list)\n",
    "\n",
    "dynamic_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20003491    [[-0.13437852188075722, -0.34900547445278896, ...\n",
       "20022095    [[4.1824562123508136, -3.0782938415824534, -0....\n",
       "20026358    [[1.3045663895297661, -1.4407208213046547, 0.4...\n",
       "20030062    [[-0.33994208065368914, 0.014899641164499634, ...\n",
       "20042619    [[-0.33994208065368914, 0.5607573145904325, -0...\n",
       "                                  ...                        \n",
       "29954601    [[0.8934392719839024, 0.37880475678178827, 0.3...\n",
       "29966553    [[-0.545505639426621, 0.19685219897314393, -0....\n",
       "29967192    [[0.0711850368921747, -0.7129105900700776, -0....\n",
       "29994296    [[-0.545505639426621, -0.7129105900700776, 1.4...\n",
       "29997500    [[-0.751069198199553, 2.7441880082941643, -0.5...\n",
       "Length: 970, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_test = dynamic_test['id'].value_counts().to_dict()\n",
    "dynamic_test = dynamic_test.sort_values(by=['id', 'charttime'])\n",
    "dynamic_test = dynamic_test.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_test['id']).agg(list)\n",
    "\n",
    "dynamic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])].copy()\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])].copy()\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultimodalDataset(static=static_train, dynamic=dynamic_train, id_lengths=id_lengths_train, notes=notes_train)\n",
    "validation_data = MultimodalDataset(static=static_val, dynamic=dynamic_val, id_lengths=id_lengths_val, notes=notes_val)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2000, shuffle=True, collate_fn=collation)\n",
    "val_loader = DataLoader(validation_data, batch_size=400, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "text_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for params in text_model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "model = LOSNetWeighted(input_size=9, out_features=1, hidden_size=128, text_model=text_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embeddings.word_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.position_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.token_type_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: pooler.dense.weight, Frozen: True\n",
      "\n",
      "Layer: pooler.dense.bias, Frozen: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.text_model.named_parameters():\n",
    "    print(f\"Layer: {name}, Frozen: {not param.requires_grad}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, predicted, actual):\n",
    "        return torch.sqrt(self.mse(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  7 09:19:25 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   49C    P2    73W / 300W |   3279MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_path = 'losses/bert-frozen-all/with-outliers/combined_regression'\n",
    "writer = SummaryWriter('tensorboard/runs/with-outliers/combined_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: [1/200]\n",
      "step: [1/4] | loss: 8.444\n",
      "step: [2/4] | loss: 7.776\n",
      "step: [3/4] | loss: 8.59\n",
      "step: [4/4] | loss: 8.211\n",
      "Training epoch loss: 8.2550\n",
      "\n",
      "validation epoch: [1/200]\n",
      "Validation epoch loss: 8.7986\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [2/200]\n",
      "step: [1/4] | loss: 7.988\n",
      "step: [2/4] | loss: 8.184\n",
      "step: [3/4] | loss: 8.144\n",
      "step: [4/4] | loss: 8.099\n",
      "Training epoch loss: 8.1036\n",
      "\n",
      "validation epoch: [2/200]\n",
      "Validation epoch loss: 8.5388\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [3/200]\n",
      "step: [1/4] | loss: 8.152\n",
      "step: [2/4] | loss: 7.706\n",
      "step: [3/4] | loss: 8.008\n",
      "step: [4/4] | loss: 7.934\n",
      "Training epoch loss: 7.9500\n",
      "\n",
      "validation epoch: [3/200]\n",
      "Validation epoch loss: 7.5382\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [4/200]\n",
      "step: [1/4] | loss: 7.747\n",
      "step: [2/4] | loss: 7.873\n",
      "step: [3/4] | loss: 7.993\n",
      "step: [4/4] | loss: 7.594\n",
      "Training epoch loss: 7.8016\n",
      "\n",
      "validation epoch: [4/200]\n",
      "Validation epoch loss: 7.9408\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [5/200]\n",
      "step: [1/4] | loss: 7.534\n",
      "step: [2/4] | loss: 7.181\n",
      "step: [3/4] | loss: 8.171\n",
      "step: [4/4] | loss: 7.759\n",
      "Training epoch loss: 7.6614\n",
      "\n",
      "validation epoch: [5/200]\n",
      "Validation epoch loss: 7.6052\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [6/200]\n",
      "step: [1/4] | loss: 7.818\n",
      "step: [2/4] | loss: 7.279\n",
      "step: [3/4] | loss: 7.805\n",
      "step: [4/4] | loss: 7.204\n",
      "Training epoch loss: 7.5268\n",
      "\n",
      "validation epoch: [6/200]\n",
      "Validation epoch loss: 7.9460\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [7/200]\n",
      "step: [1/4] | loss: 7.393\n",
      "step: [2/4] | loss: 6.982\n",
      "step: [3/4] | loss: 7.728\n",
      "step: [4/4] | loss: 7.544\n",
      "Training epoch loss: 7.4117\n",
      "\n",
      "validation epoch: [7/200]\n",
      "Validation epoch loss: 7.0540\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [8/200]\n",
      "step: [1/4] | loss: 7.105\n",
      "step: [2/4] | loss: 7.31\n",
      "step: [3/4] | loss: 8.118\n",
      "step: [4/4] | loss: 6.526\n",
      "Training epoch loss: 7.2648\n",
      "\n",
      "validation epoch: [8/200]\n",
      "Validation epoch loss: 7.5870\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [9/200]\n",
      "step: [1/4] | loss: 7.359\n",
      "step: [2/4] | loss: 7.338\n",
      "step: [3/4] | loss: 7.123\n",
      "step: [4/4] | loss: 6.934\n",
      "Training epoch loss: 7.1883\n",
      "\n",
      "validation epoch: [9/200]\n",
      "Validation epoch loss: 7.4973\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [10/200]\n",
      "step: [1/4] | loss: 7.065\n",
      "step: [2/4] | loss: 7.936\n",
      "step: [3/4] | loss: 6.401\n",
      "step: [4/4] | loss: 6.891\n",
      "Training epoch loss: 7.0734\n",
      "\n",
      "validation epoch: [10/200]\n",
      "Validation epoch loss: 7.7589\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [11/200]\n",
      "step: [1/4] | loss: 6.863\n",
      "step: [2/4] | loss: 7.048\n",
      "step: [3/4] | loss: 7.062\n",
      "step: [4/4] | loss: 7.07\n",
      "Training epoch loss: 7.0108\n",
      "\n",
      "validation epoch: [11/200]\n",
      "Validation epoch loss: 7.2360\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [12/200]\n",
      "step: [1/4] | loss: 7.053\n",
      "step: [2/4] | loss: 6.68\n",
      "step: [3/4] | loss: 7.237\n",
      "step: [4/4] | loss: 6.723\n",
      "Training epoch loss: 6.9232\n",
      "\n",
      "validation epoch: [12/200]\n",
      "Validation epoch loss: 6.9090\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [13/200]\n",
      "step: [1/4] | loss: 7.185\n",
      "step: [2/4] | loss: 6.846\n",
      "step: [3/4] | loss: 6.641\n",
      "step: [4/4] | loss: 6.755\n",
      "Training epoch loss: 6.8568\n",
      "\n",
      "validation epoch: [13/200]\n",
      "Validation epoch loss: 6.5573\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [14/200]\n",
      "step: [1/4] | loss: 6.368\n",
      "step: [2/4] | loss: 7.125\n",
      "step: [3/4] | loss: 6.735\n",
      "step: [4/4] | loss: 6.953\n",
      "Training epoch loss: 6.7953\n",
      "\n",
      "validation epoch: [14/200]\n",
      "Validation epoch loss: 6.6474\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [15/200]\n",
      "step: [1/4] | loss: 6.853\n",
      "step: [2/4] | loss: 6.568\n",
      "step: [3/4] | loss: 6.825\n",
      "step: [4/4] | loss: 6.729\n",
      "Training epoch loss: 6.7438\n",
      "\n",
      "validation epoch: [15/200]\n",
      "Validation epoch loss: 7.0307\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [16/200]\n",
      "step: [1/4] | loss: 7.307\n",
      "step: [2/4] | loss: 6.217\n",
      "step: [3/4] | loss: 6.368\n",
      "step: [4/4] | loss: 6.857\n",
      "Training epoch loss: 6.6871\n",
      "\n",
      "validation epoch: [16/200]\n",
      "Validation epoch loss: 7.5167\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [17/200]\n",
      "step: [1/4] | loss: 6.564\n",
      "step: [2/4] | loss: 6.694\n",
      "step: [3/4] | loss: 6.685\n",
      "step: [4/4] | loss: 6.688\n",
      "Training epoch loss: 6.6578\n",
      "\n",
      "validation epoch: [17/200]\n",
      "Validation epoch loss: 6.1737\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [18/200]\n",
      "step: [1/4] | loss: 6.645\n",
      "step: [2/4] | loss: 6.719\n",
      "step: [3/4] | loss: 7.021\n",
      "step: [4/4] | loss: 6.032\n",
      "Training epoch loss: 6.6042\n",
      "\n",
      "validation epoch: [18/200]\n",
      "Validation epoch loss: 7.4462\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [19/200]\n",
      "step: [1/4] | loss: 7.018\n",
      "step: [2/4] | loss: 6.1\n",
      "step: [3/4] | loss: 6.074\n",
      "step: [4/4] | loss: 7.157\n",
      "Training epoch loss: 6.5871\n",
      "\n",
      "validation epoch: [19/200]\n",
      "Validation epoch loss: 7.0554\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [20/200]\n",
      "step: [1/4] | loss: 5.976\n",
      "step: [2/4] | loss: 6.804\n",
      "step: [3/4] | loss: 6.552\n",
      "step: [4/4] | loss: 6.945\n",
      "Training epoch loss: 6.5692\n",
      "\n",
      "validation epoch: [20/200]\n",
      "Validation epoch loss: 6.5161\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [21/200]\n",
      "step: [1/4] | loss: 6.883\n",
      "step: [2/4] | loss: 6.289\n",
      "step: [3/4] | loss: 6.537\n",
      "step: [4/4] | loss: 6.49\n",
      "Training epoch loss: 6.5498\n",
      "\n",
      "validation epoch: [21/200]\n",
      "Validation epoch loss: 6.8803\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [22/200]\n",
      "step: [1/4] | loss: 6.874\n",
      "step: [2/4] | loss: 6.239\n",
      "step: [3/4] | loss: 6.412\n",
      "step: [4/4] | loss: 6.62\n",
      "Training epoch loss: 6.5363\n",
      "\n",
      "validation epoch: [22/200]\n",
      "Validation epoch loss: 6.7421\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [23/200]\n",
      "step: [1/4] | loss: 6.467\n",
      "step: [2/4] | loss: 6.301\n",
      "step: [3/4] | loss: 6.698\n",
      "step: [4/4] | loss: 6.645\n",
      "Training epoch loss: 6.5277\n",
      "\n",
      "validation epoch: [23/200]\n",
      "Validation epoch loss: 7.3441\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [24/200]\n",
      "step: [1/4] | loss: 6.476\n",
      "step: [2/4] | loss: 6.717\n",
      "step: [3/4] | loss: 6.405\n",
      "step: [4/4] | loss: 6.465\n",
      "Training epoch loss: 6.5157\n",
      "\n",
      "validation epoch: [24/200]\n",
      "Validation epoch loss: 6.6810\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [25/200]\n",
      "step: [1/4] | loss: 6.763\n",
      "step: [2/4] | loss: 6.318\n",
      "step: [3/4] | loss: 6.653\n",
      "step: [4/4] | loss: 6.278\n",
      "Training epoch loss: 6.5029\n",
      "\n",
      "validation epoch: [25/200]\n",
      "Validation epoch loss: 7.7828\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [26/200]\n",
      "step: [1/4] | loss: 6.48\n",
      "step: [2/4] | loss: 6.209\n",
      "step: [3/4] | loss: 6.962\n",
      "step: [4/4] | loss: 6.327\n",
      "Training epoch loss: 6.4944\n",
      "\n",
      "validation epoch: [26/200]\n",
      "Validation epoch loss: 6.8720\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [27/200]\n",
      "step: [1/4] | loss: 6.604\n",
      "step: [2/4] | loss: 6.733\n",
      "step: [3/4] | loss: 5.969\n",
      "step: [4/4] | loss: 6.682\n",
      "Training epoch loss: 6.4971\n",
      "\n",
      "validation epoch: [27/200]\n",
      "Validation epoch loss: 6.3376\n",
      "\n",
      "No improvement over 10 epochs\n",
      "Early stopping\n",
      "\n",
      "min training loss: 6.494426727294922\n",
      "min validation loss: 6.173686981201172\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "patience = 10\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'training epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "\n",
    "        loss = criterion(outputs, los)\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + step)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % max(1, round(len(train_loader) * 0.1)) == 0:\n",
    "            print(f'step: [{step+1}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step+1 == 1 and epoch == 1:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    writer.add_scalar('Loss/train_avg', avg_training_loss_epoch.item(), epoch)\n",
    "\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'Training epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'w') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'a') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print(f'validation epoch: [{epoch}/{epochs}]')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss_epoch = 0\n",
    "\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            packed_dynamic_X_val, notes_X_val, notes_intervals_val, los_val = val_batch\n",
    "\n",
    "            packed_dynamic_X_val = packed_dynamic_X_val.to(device)\n",
    "            los_val = los_val.to(device)\n",
    "\n",
    "            notes_X_val_gpu = []\n",
    "            for notes in notes_X_val:\n",
    "                notes_val_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "                notes_X_val_gpu.append(notes_val_gpu)\n",
    "\n",
    "            val_outputs = model(packed_dynamic_X_val, notes_X_val_gpu, notes_intervals_val)\n",
    "            val_loss = criterion(val_outputs, los_val)\n",
    "\n",
    "            writer.add_scalar('Loss/val', val_loss.item(), epoch * len(val_loader) + val_step)\n",
    "            validation_loss_epoch += val_loss\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "        writer.add_scalar('Loss/val_avg', avg_validation_loss.item(), epoch)\n",
    "        print(f'Validation epoch loss: {avg_validation_loss.item():.4f}\\n')\n",
    "        \n",
    "        if len(validation_loss) == 0 or (avg_validation_loss.item() < min(validation_loss)):\n",
    "            stagnation = 0\n",
    "            torch.save(model.state_dict(), 'saved-models/no-outliers/bert_frozen_all_combined_regression_best_model.pth')\n",
    "            print(f'new minimum validation loss')\n",
    "            print(f'model saved\\n')\n",
    "\n",
    "        else:\n",
    "            stagnation += 1\n",
    "\n",
    "        validation_loss.append(avg_validation_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'w') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        else:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'a') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        if stagnation >= patience:\n",
    "            print(f'No improvement over {patience} epochs')\n",
    "            print('Early stopping\\n')\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('===============================\\n')\n",
    "\n",
    "writer.close()\n",
    "print(f'min training loss: {min(training_loss):.4f}')\n",
    "print(f'min validation loss: {min(validation_loss):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
