{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Fully Frozen Bert Weights (Regression)\n",
    "- The patients here have not been separated based on whether they died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalDataset, LOSNetWeighted, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  6 13:41:12 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   25C    P8    17W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/regression/no-outliers/combined'\n",
    "\n",
    "static_train = pd.read_csv(f'{base_path}/static_train.csv')\n",
    "static_val = pd.read_csv(f'{base_path}/static_val.csv')\n",
    "static_test = pd.read_csv(f'{base_path}/static_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2311, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/notes_with_interval.csv')\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])]\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])]\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "dynamic_train = dynamic[dynamic['id'].isin(static_train['id'])].copy()\n",
    "dynamic_val = dynamic[dynamic['id'].isin(static_val['id'])].copy()\n",
    "dynamic_test = dynamic[dynamic['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>-0.712744</td>\n",
       "      <td>-0.319670</td>\n",
       "      <td>-1.032452</td>\n",
       "      <td>-0.650622</td>\n",
       "      <td>1.438007</td>\n",
       "      <td>-0.734532</td>\n",
       "      <td>-0.565091</td>\n",
       "      <td>0.725645</td>\n",
       "      <td>-0.900854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21792938</td>\n",
       "      <td>4/13/28 14:18</td>\n",
       "      <td>-0.931146</td>\n",
       "      <td>0.794023</td>\n",
       "      <td>0.371756</td>\n",
       "      <td>-1.520537</td>\n",
       "      <td>-0.679054</td>\n",
       "      <td>1.532324</td>\n",
       "      <td>-0.588363</td>\n",
       "      <td>-1.249342</td>\n",
       "      <td>1.369643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28398464</td>\n",
       "      <td>12/9/34 8:10</td>\n",
       "      <td>0.597668</td>\n",
       "      <td>-0.505286</td>\n",
       "      <td>-0.463178</td>\n",
       "      <td>1.959125</td>\n",
       "      <td>-0.114505</td>\n",
       "      <td>-0.683012</td>\n",
       "      <td>-0.611635</td>\n",
       "      <td>-0.351621</td>\n",
       "      <td>-0.333230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21525925</td>\n",
       "      <td>2/10/45 5:40</td>\n",
       "      <td>-0.275940</td>\n",
       "      <td>-0.134054</td>\n",
       "      <td>-1.146306</td>\n",
       "      <td>-0.541882</td>\n",
       "      <td>0.591182</td>\n",
       "      <td>-0.837571</td>\n",
       "      <td>-0.378918</td>\n",
       "      <td>0.187012</td>\n",
       "      <td>-0.758948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22576776</td>\n",
       "      <td>1/13/84 7:30</td>\n",
       "      <td>1.252874</td>\n",
       "      <td>-0.505286</td>\n",
       "      <td>-1.146306</td>\n",
       "      <td>-0.106925</td>\n",
       "      <td>0.167770</td>\n",
       "      <td>-0.631493</td>\n",
       "      <td>-0.565091</td>\n",
       "      <td>0.546101</td>\n",
       "      <td>-0.617042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0  26115624    9/7/50 0:22 -0.712744    -0.319670 -1.032452 -0.650622   \n",
       "1  21792938  4/13/28 14:18 -0.931146     0.794023  0.371756 -1.520537   \n",
       "2  28398464   12/9/34 8:10  0.597668    -0.505286 -0.463178  1.959125   \n",
       "6  21525925   2/10/45 5:40 -0.275940    -0.134054 -1.146306 -0.541882   \n",
       "7  22576776   1/13/84 7:30  1.252874    -0.505286 -1.146306 -0.106925   \n",
       "\n",
       "   chloride  creatinine   glucose    sodium  potassium  \n",
       "0  1.438007   -0.734532 -0.565091  0.725645  -0.900854  \n",
       "1 -0.679054    1.532324 -0.588363 -1.249342   1.369643  \n",
       "2 -0.114505   -0.683012 -0.611635 -0.351621  -0.333230  \n",
       "6  0.591182   -0.837571 -0.378918  0.187012  -0.758948  \n",
       "7  0.167770   -0.631493 -0.565091  0.546101  -0.617042  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic_train.loc[:, features] = scaler.fit_transform(dynamic_train[features])\n",
    "dynamic_val.loc[:, features] = scaler.transform(dynamic_val[features])\n",
    "dynamic_test.loc[:, features] = scaler.transform(dynamic_test[features])  \n",
    "\n",
    "dynamic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic train cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20008098    [[0.5976678649580875, -0.8765166023203315, -0....\n",
       "20013244    [[-0.49434217878227354, 0.6084077288686798, -0...\n",
       "20015730    [[-0.931146196278418, 0.237176646071427, -0.72...\n",
       "20020562    [[-0.931146196278418, 0.051561104672800596, -0...\n",
       "20022095    [[4.528904022423387, -3.2895186405024748, -0.7...\n",
       "                                  ...                        \n",
       "29988601    [[0.8160698737061598, -0.3196699781244522, 0.5...\n",
       "29990184    [[-0.0575381612861291, 1.9077165186590646, 0.2...\n",
       "29990494    [[0.8160698737061598, -0.690901060921705, -0.4...\n",
       "29994296    [[-0.49434217878227354, -0.8765166023203315, 1...\n",
       "29997500    [[-0.7127441875303457, 2.65017868425357, -0.53...\n",
       "Length: 2311, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_train = dynamic_train['id'].value_counts().to_dict()\n",
    "dynamic_train = dynamic_train.sort_values(by=['id', 'charttime'])\n",
    "dynamic_train = dynamic_train.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_train['id']).agg(list)\n",
    "\n",
    "dynamic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic val cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20042619    [[-0.27594017003420135, 0.4227921874700534, -0...\n",
       "20098037    [[-0.49434217878227354, 0.051561104672800596, ...\n",
       "20111879    [[-0.931146196278418, 0.237176646071427, 0.144...\n",
       "20197360    [[-0.0575381612861291, -0.13405443672582582, -...\n",
       "20226142    [[-0.49434217878227354, 1.165254353064559, -0....\n",
       "                                  ...                        \n",
       "29669406    [[0.16086384746194313, 0.237176646071427, -0.9...\n",
       "29742461    [[0.5976678649580875, -1.4333632265162106, 0.4...\n",
       "29801793    [[-0.7127441875303457, -0.8765166023203315, 0....\n",
       "29841097    [[-0.931146196278418, 0.7940232702673062, -0.7...\n",
       "29862044    [[-1.804754231270707, 0.9796388116659326, -0.6...\n",
       "Length: 257, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_val = dynamic_val['id'].value_counts().to_dict()\n",
    "dynamic_val = dynamic_val.sort_values(by=['id', 'charttime'])\n",
    "dynamic_val = dynamic_val.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_val['id']).agg(list)\n",
    "\n",
    "dynamic_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic test cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20021110    [[0.16086384746194313, 0.6084077288686798, -0....\n",
       "20030660    [[-0.931146196278418, -0.13405443672582582, -1...\n",
       "20114766    [[-0.0575381612861291, -0.690901060921705, 0.4...\n",
       "20128881    [[-0.0575381612861291, -0.3196699781244522, -0...\n",
       "20134597    [[-0.0575381612861291, -0.5052855195230787, -0...\n",
       "                                  ...                        \n",
       "29865517    [[2.5632859436907376, -2.1758253921107165, 0.2...\n",
       "29891413    [[-1.804754231270707, -0.13405443672582582, -0...\n",
       "29907670    [[-0.7127441875303457, 0.051561104672800596, -...\n",
       "29923363    [[-0.0575381612861291, -0.3196699781244522, -1...\n",
       "29970938    [[-0.27594017003420135, -0.13405443672582582, ...\n",
       "Length: 286, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_test = dynamic_test['id'].value_counts().to_dict()\n",
    "dynamic_test = dynamic_test.sort_values(by=['id', 'charttime'])\n",
    "dynamic_test = dynamic_test.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_test['id']).agg(list)\n",
    "\n",
    "dynamic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])].copy()\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])].copy()\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultimodalDataset(static=static_train, dynamic=dynamic_train, id_lengths=id_lengths_train, notes=notes_train)\n",
    "validation_data = MultimodalDataset(static=static_val, dynamic=dynamic_val, id_lengths=id_lengths_val, notes=notes_val)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=1500, shuffle=True, collate_fn=collation)\n",
    "val_loader = DataLoader(validation_data, batch_size=150, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "text_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for params in text_model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "model = LOSNetWeighted(input_size=9, out_features=1, hidden_size=128, text_model=text_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, predicted, actual):\n",
    "        return torch.sqrt(self.mse(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  6 13:41:31 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   28C    P2    62W / 300W |   2519MiB / 49140MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_path = 'losses/no-outliers/baseline_regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: [1/200]\n",
      "step: [1/2] | loss: 4.543\n",
      "step: [2/2] | loss: 4.364\n",
      "Training epoch loss: 4.4536\n",
      "\n",
      "validation epoch: [1/200]\n",
      "Validation epoch loss: 3.9128\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [2/200]\n",
      "step: [1/2] | loss: 3.908\n",
      "step: [2/2] | loss: 3.436\n",
      "Training epoch loss: 3.6722\n",
      "\n",
      "validation epoch: [2/200]\n",
      "Validation epoch loss: 3.1915\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [3/200]\n",
      "step: [1/2] | loss: 3.09\n",
      "step: [2/2] | loss: 2.943\n",
      "Training epoch loss: 3.0162\n",
      "\n",
      "validation epoch: [3/200]\n",
      "Validation epoch loss: 2.7383\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [4/200]\n",
      "step: [1/2] | loss: 2.703\n",
      "step: [2/2] | loss: 2.631\n",
      "Training epoch loss: 2.6668\n",
      "\n",
      "validation epoch: [4/200]\n",
      "Validation epoch loss: 2.7653\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [5/200]\n",
      "step: [1/2] | loss: 2.679\n",
      "step: [2/2] | loss: 2.796\n",
      "Training epoch loss: 2.7373\n",
      "\n",
      "validation epoch: [5/200]\n",
      "Validation epoch loss: 2.9586\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [6/200]\n",
      "step: [1/2] | loss: 2.863\n",
      "step: [2/2] | loss: 2.971\n",
      "Training epoch loss: 2.9170\n",
      "\n",
      "validation epoch: [6/200]\n",
      "Validation epoch loss: 3.0076\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [7/200]\n",
      "step: [1/2] | loss: 2.92\n",
      "step: [2/2] | loss: 2.988\n",
      "Training epoch loss: 2.9538\n",
      "\n",
      "validation epoch: [7/200]\n",
      "Validation epoch loss: 2.9462\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [8/200]\n",
      "step: [1/2] | loss: 2.853\n",
      "step: [2/2] | loss: 2.839\n",
      "Training epoch loss: 2.8460\n",
      "\n",
      "validation epoch: [8/200]\n",
      "Validation epoch loss: 2.8070\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [9/200]\n",
      "step: [1/2] | loss: 2.767\n",
      "step: [2/2] | loss: 2.614\n",
      "Training epoch loss: 2.6906\n",
      "\n",
      "validation epoch: [9/200]\n",
      "Validation epoch loss: 2.6822\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [10/200]\n",
      "step: [1/2] | loss: 2.68\n",
      "step: [2/2] | loss: 2.554\n",
      "Training epoch loss: 2.6171\n",
      "\n",
      "validation epoch: [10/200]\n",
      "Validation epoch loss: 2.6918\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [11/200]\n",
      "step: [1/2] | loss: 2.645\n",
      "step: [2/2] | loss: 2.657\n",
      "Training epoch loss: 2.6512\n",
      "\n",
      "validation epoch: [11/200]\n",
      "Validation epoch loss: 2.7553\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [12/200]\n",
      "step: [1/2] | loss: 2.726\n",
      "step: [2/2] | loss: 2.651\n",
      "Training epoch loss: 2.6885\n",
      "\n",
      "validation epoch: [12/200]\n",
      "Validation epoch loss: 2.7355\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [13/200]\n",
      "step: [1/2] | loss: 2.628\n",
      "step: [2/2] | loss: 2.891\n",
      "Training epoch loss: 2.7593\n",
      "\n",
      "validation epoch: [13/200]\n",
      "Validation epoch loss: 2.7629\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [14/200]\n",
      "step: [1/2] | loss: 2.69\n",
      "step: [2/2] | loss: 2.719\n",
      "Training epoch loss: 2.7046\n",
      "\n",
      "validation epoch: [14/200]\n",
      "Validation epoch loss: 2.7449\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [15/200]\n",
      "step: [1/2] | loss: 2.62\n",
      "step: [2/2] | loss: 2.719\n",
      "Training epoch loss: 2.6695\n",
      "\n",
      "validation epoch: [15/200]\n",
      "Validation epoch loss: 2.7160\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [16/200]\n",
      "step: [1/2] | loss: 2.647\n",
      "step: [2/2] | loss: 2.602\n",
      "Training epoch loss: 2.6247\n",
      "\n",
      "validation epoch: [16/200]\n",
      "Validation epoch loss: 2.6597\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [17/200]\n",
      "step: [1/2] | loss: 2.647\n",
      "step: [2/2] | loss: 2.617\n",
      "Training epoch loss: 2.6319\n",
      "\n",
      "validation epoch: [17/200]\n",
      "Validation epoch loss: 2.6769\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [18/200]\n",
      "step: [1/2] | loss: 2.637\n",
      "step: [2/2] | loss: 2.687\n",
      "Training epoch loss: 2.6619\n",
      "\n",
      "validation epoch: [18/200]\n",
      "Validation epoch loss: 2.7431\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [19/200]\n",
      "step: [1/2] | loss: 2.585\n",
      "step: [2/2] | loss: 2.792\n",
      "Training epoch loss: 2.6885\n",
      "\n",
      "validation epoch: [19/200]\n",
      "Validation epoch loss: 2.7202\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [20/200]\n",
      "step: [1/2] | loss: 2.623\n",
      "step: [2/2] | loss: 2.688\n",
      "Training epoch loss: 2.6559\n",
      "\n",
      "validation epoch: [20/200]\n",
      "Validation epoch loss: 2.6661\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [21/200]\n",
      "step: [1/2] | loss: 2.678\n",
      "step: [2/2] | loss: 2.536\n",
      "Training epoch loss: 2.6074\n",
      "\n",
      "validation epoch: [21/200]\n",
      "Validation epoch loss: 2.6691\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [22/200]\n",
      "step: [1/2] | loss: 2.657\n",
      "step: [2/2] | loss: 2.56\n",
      "Training epoch loss: 2.6086\n",
      "\n",
      "validation epoch: [22/200]\n",
      "Validation epoch loss: 2.7158\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [23/200]\n",
      "step: [1/2] | loss: 2.624\n",
      "step: [2/2] | loss: 2.635\n",
      "Training epoch loss: 2.6294\n",
      "\n",
      "validation epoch: [23/200]\n",
      "Validation epoch loss: 2.6826\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [24/200]\n",
      "step: [1/2] | loss: 2.688\n",
      "step: [2/2] | loss: 2.524\n",
      "Training epoch loss: 2.6062\n",
      "\n",
      "validation epoch: [24/200]\n",
      "Validation epoch loss: 2.7192\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [25/200]\n",
      "step: [1/2] | loss: 2.574\n",
      "step: [2/2] | loss: 2.726\n",
      "Training epoch loss: 2.6503\n",
      "\n",
      "validation epoch: [25/200]\n",
      "Validation epoch loss: 2.7346\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [26/200]\n",
      "step: [1/2] | loss: 2.544\n",
      "step: [2/2] | loss: 2.754\n",
      "Training epoch loss: 2.6490\n",
      "\n",
      "validation epoch: [26/200]\n",
      "Validation epoch loss: 2.6802\n",
      "\n",
      "No improvement over 10 epochs\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "patience = 10\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'training epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "\n",
    "        loss = criterion(outputs, los)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % max(1, round(len(train_loader) * 0.1)) == 0:\n",
    "            print(f'step: [{step+1}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step+1 == 1 and epoch == 1:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'Training epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'w') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'a') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print(f'validation epoch: [{epoch}/{epochs}]')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss_epoch = 0\n",
    "\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            packed_dynamic_X_val, notes_X_val, notes_intervals_val, los_val = val_batch\n",
    "\n",
    "            packed_dynamic_X_val = packed_dynamic_X_val.to(device)\n",
    "            los_val = los_val.to(device)\n",
    "\n",
    "            notes_X_val_gpu = []\n",
    "            for notes in notes_X_val:\n",
    "                notes_val_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "                notes_X_val_gpu.append(notes_val_gpu)\n",
    "\n",
    "            val_outputs = model(packed_dynamic_X_val, notes_X_val_gpu, notes_intervals_val)\n",
    "            val_loss = criterion(val_outputs, los_val)\n",
    "            validation_loss_epoch += val_loss\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "        print(f'Validation epoch loss: {avg_validation_loss.item():.4f}\\n')\n",
    "        \n",
    "        if len(validation_loss) == 0 or (avg_validation_loss.item() < min(validation_loss)):\n",
    "            stagnation = 0\n",
    "            torch.save(model.state_dict(), 'saved-models/no-outliers/baseline_regression_best_model.pth')\n",
    "            print(f'new minimum validation loss')\n",
    "            print(f'model saved\\n')\n",
    "\n",
    "        else:\n",
    "            stagnation += 1\n",
    "\n",
    "        validation_loss.append(avg_validation_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'w') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        else:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'a') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        if stagnation >= patience:\n",
    "            print(f'No improvement over {patience} epochs')\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('===============================\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
