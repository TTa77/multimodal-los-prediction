{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Frozen Bert Weights for Regression\n",
    "- This model trains and tests solely on patients who did not die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalDataset, LOSNetWeighted, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun Apr  7 09:19:09 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   46C    P8    19W / 300W |   1641MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/regression/with-outliers/partitioned'\n",
    "\n",
    "static_train = pd.read_csv(f'{base_path}/static_alive_train.csv')\n",
    "static_val = pd.read_csv(f'{base_path}/static_alive_val.csv')\n",
    "static_test = pd.read_csv(f'{base_path}/static_alive_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6678, 24)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/notes_cleaned.csv')\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])]\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])]\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "dynamic_train = dynamic[dynamic['id'].isin(static_train['id'])].copy()\n",
    "dynamic_val = dynamic[dynamic['id'].isin(static_val['id'])].copy()\n",
    "dynamic_test = dynamic[dynamic['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28793466</td>\n",
       "      <td>4/12/29 3:35</td>\n",
       "      <td>0.137698</td>\n",
       "      <td>-0.426118</td>\n",
       "      <td>-0.892771</td>\n",
       "      <td>0.414173</td>\n",
       "      <td>0.584317</td>\n",
       "      <td>-0.694640</td>\n",
       "      <td>0.067924</td>\n",
       "      <td>0.285866</td>\n",
       "      <td>-0.628766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>-0.746722</td>\n",
       "      <td>-0.237967</td>\n",
       "      <td>-1.007057</td>\n",
       "      <td>-0.550493</td>\n",
       "      <td>1.278780</td>\n",
       "      <td>-0.694640</td>\n",
       "      <td>-0.615583</td>\n",
       "      <td>0.643713</td>\n",
       "      <td>-0.895698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28164589</td>\n",
       "      <td>3/11/59 1:11</td>\n",
       "      <td>-0.746722</td>\n",
       "      <td>2.019844</td>\n",
       "      <td>0.859625</td>\n",
       "      <td>-0.121752</td>\n",
       "      <td>0.167640</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>-0.711513</td>\n",
       "      <td>1.538330</td>\n",
       "      <td>-1.162630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28478629</td>\n",
       "      <td>10/8/96 5:30</td>\n",
       "      <td>0.358803</td>\n",
       "      <td>0.702787</td>\n",
       "      <td>1.926300</td>\n",
       "      <td>-1.086418</td>\n",
       "      <td>-0.804607</td>\n",
       "      <td>0.558338</td>\n",
       "      <td>-0.075972</td>\n",
       "      <td>-0.429827</td>\n",
       "      <td>0.572427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25387632</td>\n",
       "      <td>5/14/48 9:25</td>\n",
       "      <td>-0.304512</td>\n",
       "      <td>0.702787</td>\n",
       "      <td>-0.588006</td>\n",
       "      <td>0.199803</td>\n",
       "      <td>-0.804607</td>\n",
       "      <td>-0.787453</td>\n",
       "      <td>-0.759479</td>\n",
       "      <td>-0.787674</td>\n",
       "      <td>-0.895698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0  28793466  4/12/29 3:35  0.137698    -0.426118 -0.892771  0.414173   \n",
       "1  26115624   9/7/50 0:22 -0.746722    -0.237967 -1.007057 -0.550493   \n",
       "2  28164589  3/11/59 1:11 -0.746722     2.019844  0.859625 -0.121752   \n",
       "7  28478629  10/8/96 5:30  0.358803     0.702787  1.926300 -1.086418   \n",
       "8  25387632  5/14/48 9:25 -0.304512     0.702787 -0.588006  0.199803   \n",
       "\n",
       "   chloride  creatinine   glucose    sodium  potassium  \n",
       "0  0.584317   -0.694640  0.067924  0.285866  -0.628766  \n",
       "1  1.278780   -0.694640 -0.615583  0.643713  -0.895698  \n",
       "2  0.167640    0.001459 -0.711513  1.538330  -1.162630  \n",
       "7 -0.804607    0.558338 -0.075972 -0.429827   0.572427  \n",
       "8 -0.804607   -0.787453 -0.759479 -0.787674  -0.895698  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic_train.loc[:, features] = scaler.fit_transform(dynamic_train[features])\n",
    "dynamic_val.loc[:, features] = scaler.transform(dynamic_val[features])\n",
    "dynamic_test.loc[:, features] = scaler.transform(dynamic_test[features])  \n",
    "\n",
    "dynamic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic train preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20001361    [[-0.3045122933429746, -0.23796719989995685, -...\n",
       "20003491    [[-0.08340719150706118, -0.426118097367006, -0...\n",
       "20008098    [[0.5799081140006791, -0.8024198923011043, -0....\n",
       "20009330    [[0.3588030121647657, -1.1787216872352027, -0....\n",
       "20011505    [[2.127643826852073, -2.495777969504547, -0.85...\n",
       "                                  ...                        \n",
       "29991038    [[0.5799081140006791, -0.8024198923011043, 1.3...\n",
       "29991539    [[0.5799081140006791, 1.0790890823693873, 0.40...\n",
       "29994296    [[-0.525617395178888, -0.8024198923011043, 1.5...\n",
       "29997500    [[-0.7467224970148014, 2.77244715957283, -0.51...\n",
       "29998399    [[-0.7467224970148014, -0.23796719989995685, -...\n",
       "Length: 6678, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_train = dynamic_train['id'].value_counts().to_dict()\n",
    "dynamic_train = dynamic_train.sort_values(by=['id', 'charttime'])\n",
    "dynamic_train = dynamic_train.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_train['id']).agg(list)\n",
    "\n",
    "dynamic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic val preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20031816    [[0.3588030121647657, 1.643541774770535, -0.39...\n",
       "20033924    [[-0.525617395178888, -0.04981630243290767, -0...\n",
       "20055925    [[-0.7467224970148014, 0.3264854925011907, -1....\n",
       "20058274    [[3.8964846415393803, -3.248381559372744, 4.89...\n",
       "20063894    [[0.3588030121647657, -0.6142689948340552, -0....\n",
       "                                  ...                        \n",
       "29911812    [[-1.1889327006866282, 0.1383345950341415, -0....\n",
       "29917727    [[0.8010132158365925, -1.1787216872352027, 2.7...\n",
       "29935333    [[-0.3045122933429746, 1.4553908773034858, -0....\n",
       "29962832    [[0.5799081140006791, -0.6142689948340552, 4.1...\n",
       "29996513    [[0.8010132158365925, -0.426118097367006, 0.32...\n",
       "Length: 743, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_val = dynamic_val['id'].value_counts().to_dict()\n",
    "dynamic_val = dynamic_val.sort_values(by=['id', 'charttime'])\n",
    "dynamic_val = dynamic_val.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_val['id']).agg(list)\n",
    "\n",
    "dynamic_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20003425    [[-1.631142904358455, 1.0790890823693873, -0.3...\n",
       "20009550    [[0.3588030121647657, 1.2672399798364367, 0.25...\n",
       "20024788    [[1.2432234195084193, -0.8024198923011043, 1.7...\n",
       "20029679    [[-0.9678275988507148, 0.1383345950341415, -0....\n",
       "20034400    [[0.13769791032885223, -0.9905707897681535, 1....\n",
       "                                  ...                        \n",
       "29932591    [[-0.9678275988507148, 2.2079944671716825, -0....\n",
       "29934368    [[0.13769791032885223, -0.426118097367006, -0....\n",
       "29941780    [[0.5799081140006791, -0.04981630243290767, 0....\n",
       "29988601    [[0.8010132158365925, -0.23796719989995685, 0....\n",
       "29993312    [[2.127643826852073, -0.6142689948340552, 2.53...\n",
       "Length: 825, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_test = dynamic_test['id'].value_counts().to_dict()\n",
    "dynamic_test = dynamic_test.sort_values(by=['id', 'charttime'])\n",
    "dynamic_test = dynamic_test.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_test['id']).agg(list)\n",
    "\n",
    "dynamic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])].copy()\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])].copy()\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultimodalDataset(static=static_train, dynamic=dynamic_train, id_lengths=id_lengths_train, notes=notes_train)\n",
    "validation_data = MultimodalDataset(static=static_val, dynamic=dynamic_val, id_lengths=id_lengths_val, notes=notes_val)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2000, shuffle=True, collate_fn=collation)\n",
    "val_loader = DataLoader(validation_data, batch_size=400, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "text_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for params in text_model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "model = LOSNetWeighted(input_size=9, out_features=1, hidden_size=128, text_model=text_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embeddings.word_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.position_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.token_type_embeddings.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: embeddings.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.0.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.1.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.2.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.3.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.4.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.5.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.6.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.7.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.8.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.9.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.10.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: encoder.layer.11.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: pooler.dense.weight, Frozen: True\n",
      "\n",
      "Layer: pooler.dense.bias, Frozen: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.text_model.named_parameters():\n",
    "    print(f\"Layer: {name}, Frozen: {not param.requires_grad}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, predicted, actual):\n",
    "        return torch.sqrt(self.mse(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun Apr  7 09:19:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   49C    P2    74W / 300W |   3941MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_path = 'losses/bert-frozen-all/with-outliers/alive_regression'\n",
    "writer = SummaryWriter('tensorboard/runs/with-outliers/alive_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: [1/200]\n",
      "step: [1/4] | loss: 7.486\n",
      "step: [2/4] | loss: 8.521\n",
      "step: [3/4] | loss: 8.5\n",
      "step: [4/4] | loss: 8.356\n",
      "Training epoch loss: 8.2158\n",
      "\n",
      "validation epoch: [1/200]\n",
      "Validation epoch loss: 7.6988\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [2/200]\n",
      "step: [1/4] | loss: 7.858\n",
      "step: [2/4] | loss: 8.22\n",
      "step: [3/4] | loss: 8.867\n",
      "step: [4/4] | loss: 7.006\n",
      "Training epoch loss: 7.9876\n",
      "\n",
      "validation epoch: [2/200]\n",
      "Validation epoch loss: 7.6922\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [3/200]\n",
      "step: [1/4] | loss: 8.19\n",
      "step: [2/4] | loss: 8.444\n",
      "step: [3/4] | loss: 8.005\n",
      "step: [4/4] | loss: 8.078\n",
      "Training epoch loss: 8.1792\n",
      "\n",
      "validation epoch: [3/200]\n",
      "Validation epoch loss: 7.6312\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [4/200]\n",
      "step: [1/4] | loss: 8.068\n",
      "step: [2/4] | loss: 8.552\n",
      "step: [3/4] | loss: 7.974\n",
      "step: [4/4] | loss: 8.193\n",
      "Training epoch loss: 8.1969\n",
      "\n",
      "validation epoch: [4/200]\n",
      "Validation epoch loss: 7.6915\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [5/200]\n",
      "step: [1/4] | loss: 8.07\n",
      "step: [2/4] | loss: 7.857\n",
      "step: [3/4] | loss: 8.45\n",
      "step: [4/4] | loss: 8.81\n",
      "Training epoch loss: 8.2968\n",
      "\n",
      "validation epoch: [5/200]\n",
      "Validation epoch loss: 7.6962\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [6/200]\n",
      "step: [1/4] | loss: 8.533\n",
      "step: [2/4] | loss: 8.208\n",
      "step: [3/4] | loss: 8.147\n",
      "step: [4/4] | loss: 7.292\n",
      "Training epoch loss: 8.0449\n",
      "\n",
      "validation epoch: [6/200]\n",
      "Validation epoch loss: 7.6777\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [7/200]\n",
      "step: [1/4] | loss: 8.103\n",
      "step: [2/4] | loss: 8.577\n",
      "step: [3/4] | loss: 8.045\n",
      "step: [4/4] | loss: 7.799\n",
      "Training epoch loss: 8.1311\n",
      "\n",
      "validation epoch: [7/200]\n",
      "Validation epoch loss: 7.6851\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [8/200]\n",
      "step: [1/4] | loss: 7.863\n",
      "step: [2/4] | loss: 8.631\n",
      "step: [3/4] | loss: 8.01\n",
      "step: [4/4] | loss: 8.431\n",
      "Training epoch loss: 8.2337\n",
      "\n",
      "validation epoch: [8/200]\n",
      "Validation epoch loss: 7.6737\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [9/200]\n",
      "step: [1/4] | loss: 7.971\n",
      "step: [2/4] | loss: 8.446\n",
      "step: [3/4] | loss: 8.222\n",
      "step: [4/4] | loss: 8.075\n",
      "Training epoch loss: 8.1784\n",
      "\n",
      "validation epoch: [9/200]\n",
      "Validation epoch loss: 7.6799\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [10/200]\n",
      "step: [1/4] | loss: 8.418\n",
      "step: [2/4] | loss: 7.951\n",
      "step: [3/4] | loss: 7.906\n",
      "step: [4/4] | loss: 9.087\n",
      "Training epoch loss: 8.3404\n",
      "\n",
      "validation epoch: [10/200]\n",
      "Validation epoch loss: 7.6844\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [11/200]\n",
      "step: [1/4] | loss: 8.196\n",
      "step: [2/4] | loss: 8.406\n",
      "step: [3/4] | loss: 8.013\n",
      "step: [4/4] | loss: 8.151\n",
      "Training epoch loss: 8.1915\n",
      "\n",
      "validation epoch: [11/200]\n",
      "Validation epoch loss: 7.5247\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [12/200]\n",
      "step: [1/4] | loss: 8.604\n",
      "step: [2/4] | loss: 8.386\n",
      "step: [3/4] | loss: 7.742\n",
      "step: [4/4] | loss: 7.733\n",
      "Training epoch loss: 8.1164\n",
      "\n",
      "validation epoch: [12/200]\n",
      "Validation epoch loss: 7.6017\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [13/200]\n",
      "step: [1/4] | loss: 7.967\n",
      "step: [2/4] | loss: 8.316\n",
      "step: [3/4] | loss: 8.382\n",
      "step: [4/4] | loss: 7.998\n",
      "Training epoch loss: 8.1657\n",
      "\n",
      "validation epoch: [13/200]\n",
      "Validation epoch loss: 7.5711\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [14/200]\n",
      "step: [1/4] | loss: 8.377\n",
      "step: [2/4] | loss: 8.02\n",
      "step: [3/4] | loss: 7.933\n",
      "step: [4/4] | loss: 8.952\n",
      "Training epoch loss: 8.3201\n",
      "\n",
      "validation epoch: [14/200]\n",
      "Validation epoch loss: 7.6956\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [15/200]\n",
      "step: [1/4] | loss: 8.06\n",
      "step: [2/4] | loss: 8.049\n",
      "step: [3/4] | loss: 8.547\n",
      "step: [4/4] | loss: 8.014\n",
      "Training epoch loss: 8.1674\n",
      "\n",
      "validation epoch: [15/200]\n",
      "Validation epoch loss: 7.5939\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [16/200]\n",
      "step: [1/4] | loss: 8.043\n",
      "step: [2/4] | loss: 8.102\n",
      "step: [3/4] | loss: 8.308\n",
      "step: [4/4] | loss: 8.625\n",
      "Training epoch loss: 8.2696\n",
      "\n",
      "validation epoch: [16/200]\n",
      "Validation epoch loss: 7.6621\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [17/200]\n",
      "step: [1/4] | loss: 7.864\n",
      "step: [2/4] | loss: 8.373\n",
      "step: [3/4] | loss: 8.37\n",
      "step: [4/4] | loss: 8.159\n",
      "Training epoch loss: 8.1914\n",
      "\n",
      "validation epoch: [17/200]\n",
      "Validation epoch loss: 7.7005\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [18/200]\n",
      "step: [1/4] | loss: 8.312\n",
      "step: [2/4] | loss: 7.981\n",
      "step: [3/4] | loss: 8.531\n",
      "step: [4/4] | loss: 7.49\n",
      "Training epoch loss: 8.0783\n",
      "\n",
      "validation epoch: [18/200]\n",
      "Validation epoch loss: 7.7004\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [19/200]\n",
      "step: [1/4] | loss: 8.656\n",
      "step: [2/4] | loss: 8.149\n",
      "step: [3/4] | loss: 7.898\n",
      "step: [4/4] | loss: 7.844\n",
      "Training epoch loss: 8.1368\n",
      "\n",
      "validation epoch: [19/200]\n",
      "Validation epoch loss: 7.6187\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [20/200]\n",
      "step: [1/4] | loss: 7.897\n",
      "step: [2/4] | loss: 8.103\n",
      "step: [3/4] | loss: 8.36\n",
      "step: [4/4] | loss: 8.867\n",
      "Training epoch loss: 8.3069\n",
      "\n",
      "validation epoch: [20/200]\n",
      "Validation epoch loss: 7.6999\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [21/200]\n",
      "step: [1/4] | loss: 8.047\n",
      "step: [2/4] | loss: 8.292\n",
      "step: [3/4] | loss: 8.26\n",
      "step: [4/4] | loss: 8.206\n",
      "Training epoch loss: 8.2014\n",
      "\n",
      "validation epoch: [21/200]\n",
      "Validation epoch loss: 7.6993\n",
      "\n",
      "No improvement over 10 epochs\n",
      "Early stopping\n",
      "\n",
      "min training loss: 7.9876\n",
      "min validation loss: 7.5247\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "patience = 10\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'training epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "\n",
    "        loss = criterion(outputs, los)\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + step)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % max(1, round(len(train_loader) * 0.1)) == 0:\n",
    "            print(f'step: [{step+1}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step+1 == 1 and epoch == 1:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    writer.add_scalar('Loss/train_avg', avg_training_loss_epoch.item(), epoch)\n",
    "\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'Training epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'w') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'a') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print(f'validation epoch: [{epoch}/{epochs}]')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss_epoch = 0\n",
    "\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            packed_dynamic_X_val, notes_X_val, notes_intervals_val, los_val = val_batch\n",
    "\n",
    "            packed_dynamic_X_val = packed_dynamic_X_val.to(device)\n",
    "            los_val = los_val.to(device)\n",
    "\n",
    "            notes_X_val_gpu = []\n",
    "            for notes in notes_X_val:\n",
    "                notes_val_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "                notes_X_val_gpu.append(notes_val_gpu)\n",
    "\n",
    "            val_outputs = model(packed_dynamic_X_val, notes_X_val_gpu, notes_intervals_val)\n",
    "            val_loss = criterion(val_outputs, los_val)\n",
    "\n",
    "            writer.add_scalar('Loss/val', val_loss.item(), epoch * len(val_loader) + val_step)\n",
    "            validation_loss_epoch += val_loss\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "        writer.add_scalar('Loss/val_avg', avg_validation_loss.item(), epoch)\n",
    "        print(f'Validation epoch loss: {avg_validation_loss.item():.4f}\\n')\n",
    "        \n",
    "        if len(validation_loss) == 0 or (avg_validation_loss.item() < min(validation_loss)):\n",
    "            stagnation = 0\n",
    "            torch.save(model.state_dict(), 'saved-models/no-outliers/bert_frozen_all_alive_regression_best_model.pth')\n",
    "            print(f'new minimum validation loss')\n",
    "            print(f'model saved\\n')\n",
    "\n",
    "        else:\n",
    "            stagnation += 1\n",
    "\n",
    "        validation_loss.append(avg_validation_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'w') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        else:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'a') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        if stagnation >= patience:\n",
    "            print(f'No improvement over {patience} epochs')\n",
    "            print('Early stopping\\n')\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('===============================\\n')\n",
    "\n",
    "writer.close()\n",
    "print(f'min training loss: {min(training_loss):.4f}')\n",
    "print(f'min validation loss: {min(validation_loss):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
