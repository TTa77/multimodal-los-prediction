{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Frozen Bert Weights for Regression\n",
    "- This model trains and tests solely on patients who did not die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from multimodal import MultimodalDataset, LOSNetWeighted, collation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  7 03:45:01 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   38C    P8    21W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/regression/no-outliers/partitioned'\n",
    "\n",
    "static_train = pd.read_csv(f'{base_path}/static_alive_train.csv')\n",
    "static_val = pd.read_csv(f'{base_path}/static_alive_val.csv')\n",
    "static_test = pd.read_csv(f'{base_path}/static_alive_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('data/notes_with_interval.csv')\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])]\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])]\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = pd.read_csv('data/dynamic_cleaned.csv')\n",
    "dynamic_train = dynamic[dynamic['id'].isin(static_train['id'])].copy()\n",
    "dynamic_val = dynamic[dynamic['id'].isin(static_val['id'])].copy()\n",
    "dynamic_test = dynamic[dynamic['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26115624</td>\n",
       "      <td>9/7/50 0:22</td>\n",
       "      <td>-0.708463</td>\n",
       "      <td>-0.374389</td>\n",
       "      <td>-0.996759</td>\n",
       "      <td>-0.690290</td>\n",
       "      <td>1.445426</td>\n",
       "      <td>-0.732283</td>\n",
       "      <td>-0.544027</td>\n",
       "      <td>0.747069</td>\n",
       "      <td>-0.907270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21525925</td>\n",
       "      <td>2/10/45 5:40</td>\n",
       "      <td>-0.239813</td>\n",
       "      <td>-0.181126</td>\n",
       "      <td>-1.108497</td>\n",
       "      <td>-0.580698</td>\n",
       "      <td>0.587007</td>\n",
       "      <td>-0.835441</td>\n",
       "      <td>-0.360288</td>\n",
       "      <td>0.194625</td>\n",
       "      <td>-0.762188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20866960</td>\n",
       "      <td>1/16/20 0:00</td>\n",
       "      <td>-0.708463</td>\n",
       "      <td>-0.567653</td>\n",
       "      <td>-0.326331</td>\n",
       "      <td>-1.457436</td>\n",
       "      <td>0.873147</td>\n",
       "      <td>-0.629125</td>\n",
       "      <td>0.581379</td>\n",
       "      <td>-0.173670</td>\n",
       "      <td>-0.181863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21580342</td>\n",
       "      <td>7/24/31 6:20</td>\n",
       "      <td>0.228836</td>\n",
       "      <td>0.785193</td>\n",
       "      <td>-0.065609</td>\n",
       "      <td>1.720741</td>\n",
       "      <td>-0.557553</td>\n",
       "      <td>2.001405</td>\n",
       "      <td>-0.429190</td>\n",
       "      <td>0.010477</td>\n",
       "      <td>-0.762188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>23640433</td>\n",
       "      <td>8/11/73 13:47</td>\n",
       "      <td>-0.942788</td>\n",
       "      <td>1.558249</td>\n",
       "      <td>-0.549807</td>\n",
       "      <td>-0.251921</td>\n",
       "      <td>-0.557553</td>\n",
       "      <td>1.537194</td>\n",
       "      <td>-0.452157</td>\n",
       "      <td>0.562921</td>\n",
       "      <td>-0.617107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      charttime  aniongap  bicarbonate       bun   calcium  \\\n",
       "0   26115624    9/7/50 0:22 -0.708463    -0.374389 -0.996759 -0.690290   \n",
       "6   21525925   2/10/45 5:40 -0.239813    -0.181126 -1.108497 -0.580698   \n",
       "10  20866960   1/16/20 0:00 -0.708463    -0.567653 -0.326331 -1.457436   \n",
       "12  21580342   7/24/31 6:20  0.228836     0.785193 -0.065609  1.720741   \n",
       "14  23640433  8/11/73 13:47 -0.942788     1.558249 -0.549807 -0.251921   \n",
       "\n",
       "    chloride  creatinine   glucose    sodium  potassium  \n",
       "0   1.445426   -0.732283 -0.544027  0.747069  -0.907270  \n",
       "6   0.587007   -0.835441 -0.360288  0.194625  -0.762188  \n",
       "10  0.873147   -0.629125  0.581379 -0.173670  -0.181863  \n",
       "12 -0.557553    2.001405 -0.429190  0.010477  -0.762188  \n",
       "14 -0.557553    1.537194 -0.452157  0.562921  -0.617107  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['aniongap', 'bicarbonate', 'bun', 'calcium', 'chloride', 'creatinine', 'glucose', 'sodium', 'potassium']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dynamic_train.loc[:, features] = scaler.fit_transform(dynamic_train[features])\n",
    "dynamic_val.loc[:, features] = scaler.transform(dynamic_val[features])\n",
    "dynamic_test.loc[:, features] = scaler.transform(dynamic_test[features])  \n",
    "\n",
    "dynamic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic train cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20008098    [[0.6974861412093623, -0.9541809330660428, -0....\n",
       "20013244    [[-0.4741383268767333, 0.5919296400622551, -0....\n",
       "20015730    [[-0.9427881141111716, 0.20540199678018062, -0...\n",
       "20021110    [[0.22883635397492402, 0.5919296400622551, -0....\n",
       "20024177    [[0.6974861412093623, 0.012138175139143398, -0...\n",
       "                                  ...                        \n",
       "29988601    [[0.9318110348265813, -0.37438946814293106, 0....\n",
       "29990184    [[-0.005488539642295093, 1.9447763915495158, 0...\n",
       "29990494    [[0.9318110348265813, -0.7609171114250055, -0....\n",
       "29994296    [[-0.4741383268767333, -0.9541809330660428, 1....\n",
       "29997500    [[-0.7084632204939524, 2.7178316781136647, -0....\n",
       "Length: 1936, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_train = dynamic_train['id'].value_counts().to_dict()\n",
    "dynamic_train = dynamic_train.sort_values(by=['id', 'charttime'])\n",
    "dynamic_train = dynamic_train.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_train['id']).agg(list)\n",
    "\n",
    "dynamic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic val cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20031816    [[0.4631612475921431, 1.5582487482674412, -0.4...\n",
       "20055925    [[-0.7084632204939524, 0.20540199678018062, -1...\n",
       "20084622    [[0.22883635397492402, 0.9784572833443296, 0.3...\n",
       "20099764    [[-0.005488539642295093, -0.7609171114250055, ...\n",
       "20161146    [[0.9318110348265813, -1.7272362196301916, 2.5...\n",
       "                                  ...                        \n",
       "29457834    [[1.1661359284438004, -0.7609171114250055, 1.3...\n",
       "29519314    [[-1.1771130077283907, -0.18112564650189383, 1...\n",
       "29598220    [[0.6974861412093623, -1.3407085763481172, 0.3...\n",
       "29744920    [[-1.1771130077283907, -0.37438946814293106, -...\n",
       "29793270    [[-0.2398134332595142, -0.18112564650189383, -...\n",
       "Length: 216, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_val = dynamic_val['id'].value_counts().to_dict()\n",
    "dynamic_val = dynamic_val.sort_values(by=['id', 'charttime'])\n",
    "dynamic_val = dynamic_val.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_val['id']).agg(list)\n",
    "\n",
    "dynamic_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic test cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20020562    [[-0.9427881141111716, 0.012138175139143398, -...\n",
       "20026358    [[1.6347857156782386, -1.7272362196301916, 0.4...\n",
       "20048978    [[1.1661359284438004, -0.9541809330660428, 1.1...\n",
       "20076522    [[-0.2398134332595142, 0.20540199678018062, -1...\n",
       "20099311    [[-0.4741383268767333, 0.5919296400622551, -1....\n",
       "                                  ...                        \n",
       "29756347    [[1.1661359284438004, -0.9541809330660428, 0.5...\n",
       "29800204    [[0.9318110348265813, 0.9784572833443296, 0.45...\n",
       "29855398    [[-0.9427881141111716, -0.18112564650189383, -...\n",
       "29921759    [[-0.9427881141111716, 0.7851934617032923, -0....\n",
       "29961239    [[-0.9427881141111716, 1.1717211049853669, -0....\n",
       "Length: 240, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lengths_test = dynamic_test['id'].value_counts().to_dict()\n",
    "dynamic_test = dynamic_test.sort_values(by=['id', 'charttime'])\n",
    "dynamic_test = dynamic_test.apply(lambda x: list(x[features]), axis=1).groupby(dynamic_test['id']).agg(list)\n",
    "\n",
    "dynamic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[['id', 'charttime', 'text', 'interval']]\n",
    "\n",
    "notes_train = notes[notes['id'].isin(static_train['id'])].copy()\n",
    "notes_val = notes[notes['id'].isin(static_val['id'])].copy()\n",
    "notes_test = notes[notes['id'].isin(static_test['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultimodalDataset(static=static_train, dynamic=dynamic_train, id_lengths=id_lengths_train, notes=notes_train)\n",
    "validation_data = MultimodalDataset(static=static_val, dynamic=dynamic_val, id_lengths=id_lengths_val, notes=notes_val)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=collation)\n",
    "val_loader = DataLoader(validation_data, batch_size=2, shuffle=True, collate_fn=collation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "text_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for layer in text_model.encoder.layer[:11]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = LOSNetWeighted(input_size=9, out_features=1, hidden_size=128, text_model=text_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: time_series_model.weight_ih_l0, Frozen: False\n",
      "\n",
      "Layer: time_series_model.weight_hh_l0, Frozen: False\n",
      "\n",
      "Layer: time_series_model.bias_ih_l0, Frozen: False\n",
      "\n",
      "Layer: time_series_model.bias_hh_l0, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.word_embeddings.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.position_embeddings.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.token_type_embeddings.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.LayerNorm.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.embeddings.LayerNorm.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.0.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.1.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.2.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.3.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.4.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.5.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.6.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.7.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.8.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.9.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.query.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.query.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.key.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.key.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.value.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.self.value.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.attention.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.intermediate.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.intermediate.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.dense.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.dense.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.LayerNorm.weight, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.10.output.LayerNorm.bias, Frozen: True\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.query.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.query.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.key.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.key.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.value.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.self.value.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.dense.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.LayerNorm.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.attention.output.LayerNorm.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.intermediate.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.intermediate.dense.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.dense.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.LayerNorm.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.encoder.layer.11.output.LayerNorm.bias, Frozen: False\n",
      "\n",
      "Layer: text_model.pooler.dense.weight, Frozen: False\n",
      "\n",
      "Layer: text_model.pooler.dense.bias, Frozen: False\n",
      "\n",
      "Layer: fc.0.weight, Frozen: False\n",
      "\n",
      "Layer: fc.0.bias, Frozen: False\n",
      "\n",
      "Layer: fc.1.weight, Frozen: False\n",
      "\n",
      "Layer: fc.1.bias, Frozen: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Frozen: {not param.requires_grad}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, predicted, actual):\n",
    "        return torch.sqrt(self.mse(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  7 03:45:33 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   39C    P2    71W / 300W |   1641MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_path = 'losses/bert-frozen-10/no-outliers/alive_regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: [1/200]\n",
      "step: [1/968] | loss: 6.651\n",
      "step: [98/968] | loss: 2.403\n",
      "step: [195/968] | loss: 2.261\n",
      "step: [292/968] | loss: 2.527\n",
      "step: [389/968] | loss: 6.442\n",
      "step: [486/968] | loss: 3.403\n",
      "step: [583/968] | loss: 4.92\n",
      "step: [680/968] | loss: 3.349\n",
      "step: [777/968] | loss: 1.635\n",
      "step: [874/968] | loss: 0.8898\n",
      "Training epoch loss: 3.5256\n",
      "\n",
      "validation epoch: [1/200]\n",
      "Validation epoch loss: 2.2113\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [2/200]\n",
      "step: [1/968] | loss: 1.059\n",
      "step: [98/968] | loss: 1.838\n",
      "step: [195/968] | loss: 1.066\n",
      "step: [292/968] | loss: 1.842\n",
      "step: [389/968] | loss: 1.681\n",
      "step: [486/968] | loss: 0.5209\n",
      "step: [583/968] | loss: 2.777\n",
      "step: [680/968] | loss: 2.086\n",
      "step: [777/968] | loss: 0.5892\n",
      "step: [874/968] | loss: 2.077\n",
      "Training epoch loss: 1.9859\n",
      "\n",
      "validation epoch: [2/200]\n",
      "Validation epoch loss: 2.1974\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [3/200]\n",
      "step: [1/968] | loss: 1.311\n",
      "step: [98/968] | loss: 2.9\n",
      "step: [195/968] | loss: 1.23\n",
      "step: [292/968] | loss: 0.672\n",
      "step: [389/968] | loss: 0.484\n",
      "step: [486/968] | loss: 2.976\n",
      "step: [583/968] | loss: 4.972\n",
      "step: [680/968] | loss: 4.515\n",
      "step: [777/968] | loss: 0.9497\n",
      "step: [874/968] | loss: 1.581\n",
      "Training epoch loss: 1.9651\n",
      "\n",
      "validation epoch: [3/200]\n",
      "Validation epoch loss: 2.1450\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [4/200]\n",
      "step: [1/968] | loss: 1.89\n",
      "step: [98/968] | loss: 1.045\n",
      "step: [195/968] | loss: 2.618\n",
      "step: [292/968] | loss: 1.028\n",
      "step: [389/968] | loss: 1.134\n",
      "step: [486/968] | loss: 1.634\n",
      "step: [583/968] | loss: 1.411\n",
      "step: [680/968] | loss: 2.941\n",
      "step: [777/968] | loss: 0.8363\n",
      "step: [874/968] | loss: 2.325\n",
      "Training epoch loss: 1.9468\n",
      "\n",
      "validation epoch: [4/200]\n",
      "Validation epoch loss: 2.1516\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [5/200]\n",
      "step: [1/968] | loss: 1.879\n",
      "step: [98/968] | loss: 1.239\n",
      "step: [195/968] | loss: 1.443\n",
      "step: [292/968] | loss: 2.353\n",
      "step: [389/968] | loss: 1.211\n",
      "step: [486/968] | loss: 0.8382\n",
      "step: [583/968] | loss: 0.3102\n",
      "step: [680/968] | loss: 1.129\n",
      "step: [777/968] | loss: 0.5364\n",
      "step: [874/968] | loss: 0.8757\n",
      "Training epoch loss: 1.9248\n",
      "\n",
      "validation epoch: [5/200]\n",
      "Validation epoch loss: 2.0912\n",
      "\n",
      "new minimum validation loss\n",
      "model saved\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [6/200]\n",
      "step: [1/968] | loss: 0.4582\n",
      "step: [98/968] | loss: 0.7325\n",
      "step: [195/968] | loss: 3.52\n",
      "step: [292/968] | loss: 1.219\n",
      "step: [389/968] | loss: 1.024\n",
      "step: [486/968] | loss: 0.9119\n",
      "step: [583/968] | loss: 1.265\n",
      "step: [680/968] | loss: 2.35\n",
      "step: [777/968] | loss: 4.083\n",
      "step: [874/968] | loss: 1.256\n",
      "Training epoch loss: 1.8904\n",
      "\n",
      "validation epoch: [6/200]\n",
      "Validation epoch loss: 2.1207\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [7/200]\n",
      "step: [1/968] | loss: 0.4425\n",
      "step: [98/968] | loss: 0.8038\n",
      "step: [195/968] | loss: 0.7309\n",
      "step: [292/968] | loss: 3.429\n",
      "step: [389/968] | loss: 0.9534\n",
      "step: [486/968] | loss: 5.564\n",
      "step: [583/968] | loss: 0.5862\n",
      "step: [680/968] | loss: 1.545\n",
      "step: [777/968] | loss: 1.897\n",
      "step: [874/968] | loss: 0.6904\n",
      "Training epoch loss: 1.8463\n",
      "\n",
      "validation epoch: [7/200]\n",
      "Validation epoch loss: 2.1100\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [8/200]\n",
      "step: [1/968] | loss: 2.191\n",
      "step: [98/968] | loss: 4.395\n",
      "step: [195/968] | loss: 0.2598\n",
      "step: [292/968] | loss: 1.823\n",
      "step: [389/968] | loss: 1.913\n",
      "step: [486/968] | loss: 4.411\n",
      "step: [583/968] | loss: 2.544\n",
      "step: [680/968] | loss: 2.856\n",
      "step: [777/968] | loss: 1.17\n",
      "step: [874/968] | loss: 3.318\n",
      "Training epoch loss: 1.8165\n",
      "\n",
      "validation epoch: [8/200]\n",
      "Validation epoch loss: 2.1013\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [9/200]\n",
      "step: [1/968] | loss: 1.773\n",
      "step: [98/968] | loss: 1.451\n",
      "step: [195/968] | loss: 1.317\n",
      "step: [292/968] | loss: 1.092\n",
      "step: [389/968] | loss: 2.467\n",
      "step: [486/968] | loss: 0.7881\n",
      "step: [583/968] | loss: 2.191\n",
      "step: [680/968] | loss: 4.034\n",
      "step: [777/968] | loss: 0.5192\n",
      "step: [874/968] | loss: 1.207\n",
      "Training epoch loss: 1.7674\n",
      "\n",
      "validation epoch: [9/200]\n",
      "Validation epoch loss: 2.1054\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [10/200]\n",
      "step: [1/968] | loss: 2.358\n",
      "step: [98/968] | loss: 1.15\n",
      "step: [195/968] | loss: 0.3553\n",
      "step: [292/968] | loss: 2.896\n",
      "step: [389/968] | loss: 2.161\n",
      "step: [486/968] | loss: 1.053\n",
      "step: [583/968] | loss: 1.385\n",
      "step: [680/968] | loss: 1.134\n",
      "step: [777/968] | loss: 1.154\n",
      "step: [874/968] | loss: 1.403\n",
      "Training epoch loss: 1.7059\n",
      "\n",
      "validation epoch: [10/200]\n",
      "Validation epoch loss: 2.1196\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [11/200]\n",
      "step: [1/968] | loss: 1.228\n",
      "step: [98/968] | loss: 2.052\n",
      "step: [195/968] | loss: 0.8239\n",
      "step: [292/968] | loss: 0.3262\n",
      "step: [389/968] | loss: 1.186\n",
      "step: [486/968] | loss: 4.105\n",
      "step: [583/968] | loss: 1.302\n",
      "step: [680/968] | loss: 0.9526\n",
      "step: [777/968] | loss: 1.17\n",
      "step: [874/968] | loss: 0.7152\n",
      "Training epoch loss: 1.6459\n",
      "\n",
      "validation epoch: [11/200]\n",
      "Validation epoch loss: 2.1708\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [12/200]\n",
      "step: [1/968] | loss: 1.467\n",
      "step: [98/968] | loss: 1.281\n",
      "step: [195/968] | loss: 0.8689\n",
      "step: [292/968] | loss: 2.838\n",
      "step: [389/968] | loss: 1.144\n",
      "step: [486/968] | loss: 0.889\n",
      "step: [583/968] | loss: 0.7451\n",
      "step: [680/968] | loss: 1.332\n",
      "step: [777/968] | loss: 0.5544\n",
      "step: [874/968] | loss: 5.135\n",
      "Training epoch loss: 1.5670\n",
      "\n",
      "validation epoch: [12/200]\n",
      "Validation epoch loss: 2.1791\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [13/200]\n",
      "step: [1/968] | loss: 1.233\n",
      "step: [98/968] | loss: 0.7095\n",
      "step: [195/968] | loss: 1.82\n",
      "step: [292/968] | loss: 2.272\n",
      "step: [389/968] | loss: 1.282\n",
      "step: [486/968] | loss: 1.608\n",
      "step: [583/968] | loss: 0.5213\n",
      "step: [680/968] | loss: 1.788\n",
      "step: [777/968] | loss: 0.6801\n",
      "step: [874/968] | loss: 3.918\n",
      "Training epoch loss: 1.5302\n",
      "\n",
      "validation epoch: [13/200]\n",
      "Validation epoch loss: 2.1752\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [14/200]\n",
      "step: [1/968] | loss: 0.9199\n",
      "step: [98/968] | loss: 1.82\n",
      "step: [195/968] | loss: 4.245\n",
      "step: [292/968] | loss: 0.4137\n",
      "step: [389/968] | loss: 1.773\n",
      "step: [486/968] | loss: 0.5275\n",
      "step: [583/968] | loss: 1.215\n",
      "step: [680/968] | loss: 0.2043\n",
      "step: [777/968] | loss: 0.4976\n",
      "step: [874/968] | loss: 1.382\n",
      "Training epoch loss: 1.5040\n",
      "\n",
      "validation epoch: [14/200]\n",
      "Validation epoch loss: 2.1927\n",
      "\n",
      "===============================\n",
      "\n",
      "training epoch: [15/200]\n",
      "step: [1/968] | loss: 0.4806\n",
      "step: [98/968] | loss: 0.9735\n",
      "step: [195/968] | loss: 0.3403\n",
      "step: [292/968] | loss: 0.453\n",
      "step: [389/968] | loss: 1.911\n",
      "step: [486/968] | loss: 0.7695\n",
      "step: [583/968] | loss: 1.133\n",
      "step: [680/968] | loss: 1.086\n",
      "step: [777/968] | loss: 0.945\n",
      "step: [874/968] | loss: 0.8736\n",
      "Training epoch loss: 1.4427\n",
      "\n",
      "validation epoch: [15/200]\n",
      "Validation epoch loss: 2.1941\n",
      "\n",
      "No improvement over 10 epochs\n",
      "Early stopping\n",
      "\n",
      "min training loss: 1.4427\n",
      "min validation loss: 2.0912\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "patience = 10\n",
    "stagnation = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'training epoch: [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    training_loss_epoch = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        packed_dynamic_X, notes_X, notes_intervals, los = batch\n",
    "\n",
    "        packed_dynamic_X = packed_dynamic_X.to(device)\n",
    "        los = los.to(device)\n",
    "\n",
    "        notes_X_gpu = []\n",
    "        for notes in notes_X:\n",
    "            notes_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "            notes_X_gpu.append(notes_gpu)\n",
    "\n",
    "        outputs = model(packed_dynamic_X, notes_X_gpu, notes_intervals)\n",
    "\n",
    "        loss = criterion(outputs, los)\n",
    "        training_loss_epoch += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % max(1, round(len(train_loader) * 0.1)) == 0:\n",
    "            print(f'step: [{step+1}/{len(train_loader)}] | loss: {loss.item():.4}')\n",
    "\n",
    "            if step+1 == 1 and epoch == 1:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'w') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "            else:\n",
    "                with open(f'{loss_base_path}/loss_step.txt', 'a') as loss_step_f:\n",
    "                    loss_step_f.write(f'{loss.item():.4f}\\n')\n",
    "\n",
    "    avg_training_loss_epoch = training_loss_epoch / len(train_loader)\n",
    "    training_loss.append(avg_training_loss_epoch.item())\n",
    "    print(f'Training epoch loss: {avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    if epoch == 1:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'w') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    else:\n",
    "        with open(f'{loss_base_path}/training_loss_epoch.txt', 'a') as loss_epoch_train_f:\n",
    "            loss_epoch_train_f.write(f'{avg_training_loss_epoch.item():.4f}\\n')\n",
    "\n",
    "    print(f'validation epoch: [{epoch}/{epochs}]')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss_epoch = 0\n",
    "\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            packed_dynamic_X_val, notes_X_val, notes_intervals_val, los_val = val_batch\n",
    "\n",
    "            packed_dynamic_X_val = packed_dynamic_X_val.to(device)\n",
    "            los_val = los_val.to(device)\n",
    "\n",
    "            notes_X_val_gpu = []\n",
    "            for notes in notes_X_val:\n",
    "                notes_val_gpu = {key: value.to(device) for key, value in notes.items()}\n",
    "                notes_X_val_gpu.append(notes_val_gpu)\n",
    "\n",
    "            val_outputs = model(packed_dynamic_X_val, notes_X_val_gpu, notes_intervals_val)\n",
    "            val_loss = criterion(val_outputs, los_val)\n",
    "            validation_loss_epoch += val_loss\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "        print(f'Validation epoch loss: {avg_validation_loss.item():.4f}\\n')\n",
    "        \n",
    "        if len(validation_loss) == 0 or (avg_validation_loss.item() < min(validation_loss)):\n",
    "            stagnation = 0\n",
    "            torch.save(model.state_dict(), 'saved-models/no-outliers/bert_frozen_10_alive_regression_best_model.pth')\n",
    "            print(f'new minimum validation loss')\n",
    "            print(f'model saved\\n')\n",
    "\n",
    "        else:\n",
    "            stagnation += 1\n",
    "\n",
    "        validation_loss.append(avg_validation_loss.item())\n",
    "\n",
    "        if epoch == 1:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'w') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        else:\n",
    "            with open(f'{loss_base_path}/validation_loss_epoch.txt', 'a') as loss_epoch_val_f:\n",
    "                loss_epoch_val_f.write(f'{avg_validation_loss.item():.4f}\\n')\n",
    "\n",
    "        if stagnation >= patience:\n",
    "            print(f'No improvement over {patience} epochs')\n",
    "            print('Early stopping\\n')\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('===============================\\n')\n",
    "\n",
    "print(f'min training loss: {min(training_loss):.4f}')\n",
    "print(f'min validation loss: {min(validation_loss):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
